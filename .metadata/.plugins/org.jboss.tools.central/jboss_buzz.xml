<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Capture Oracle database events in Apache Kafka with Debezium</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rcq4W10L08g/" /><category term="Big Data" /><category term="Event-Driven" /><category term="Stream Processing" /><category term="Apache Kafka" /><category term="change data capture" /><category term="debezium" /><category term="Oracle Database" /><author><name>Hugo Guerrero</name></author><id>https://developers.redhat.com/blog/?p=885787</id><updated>2021-04-19T07:00:22Z</updated><published>2021-04-19T07:00:22Z</published><content type="html">&lt;p&gt;One of the most requested connector plug-ins is coming to &lt;a target="_blank" rel="nofollow" href="/integration"&gt;Red Hat Integration.&lt;/a&gt; You can now stream your data from Oracle databases with the Debezium connector for Oracle in developer preview.&lt;/p&gt; &lt;p&gt;With this new connector, developers can leverage the power of the 100% open source &lt;a target="_blank" rel="nofollow" href="https://debezium.io/"&gt;Debezium project&lt;/a&gt; to stream their Oracle data to &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/resources/amq-streams-datasheet"&gt;Red Hat AMQ Streams&lt;/a&gt; &lt;a target="_blank" rel="nofollow" href="/topics/kafka-kubernetes"&gt;Apache Kafka clusters&lt;/a&gt; with Red Hat Integration.&lt;/p&gt; &lt;p&gt;This article gives an overview of the Debezium connector for Oracle. It takes you through the steps to get started streaming your Oracle database, so you can achieve &amp;#8220;&lt;a target="_blank" rel="nofollow" href="https://twitter.com/gunnarmorling/status/1123191912800845825"&gt;liberation for your data&lt;/a&gt;.&amp;#8221;&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Red Hat encourages you to use and provide feedback on the features included in developer preview offerings. However, please be advised you should not deploy them in production environments to run business-critical processes, as these releases require further stabilization and testing.&lt;/p&gt; &lt;h2&gt;An easier way to access Oracle data&lt;/h2&gt; &lt;p&gt;For many organizations, Oracle Database is the heart of their data assets. Over decades, developers have created system after system on top of this central piece of architecture. Almost every developer has faced the requirement to store enterprise information in the Oracle database. Getting the most out of the data stored there becomes critical for every business.&lt;/p&gt; &lt;p&gt;Systems distributed across hybrid clouds must process large volumes of data. The &lt;a target="_blank" rel="nofollow" href="https://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/"&gt;complexity&lt;/a&gt; of that data makes it &lt;a target="_blank" rel="nofollow" href="https://whatis.techtarget.com/definition/data-gravity"&gt;difficult to work with&lt;/a&gt; and requires the creation of specialized data pipelines. These days, it&amp;#8217;s common for business applications to require access to the latest updates to the data stored in an ORDERS or CUSTOMERS table as soon as they occur. In the past, there were few options to create this type of data pipeline. They included querying the data through Java Database Connectivity (JDBC) and using expensive proprietary technologies to capture those changes.&lt;/p&gt; &lt;h2&gt;Streaming your database with the Debezium connector for Oracle&lt;/h2&gt; &lt;p&gt;Debezium is a set of distributed services that captures row-level database changes so that applications can see and respond to them. Debezium connectors record all events to a Red Hat AMQ Streams Kafka cluster. Applications use AMQ Streams to consume change events. The Debezium approach to change data capture (CDC) uses the transaction log to &lt;a href="https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/"&gt;avoid dual writes&lt;/a&gt; from applications, which might lead to inconsistent data. Debezium is designed to use the &lt;a target="_blank" rel="nofollow" href="https://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/"&gt;database transaction log&lt;/a&gt; so that it will emit a record of every change event. By contrast, with a polling-based approach, it&amp;#8217;s possible to miss changes between queries.&lt;/p&gt; &lt;p&gt;The connector can monitor and record the row-level changes in databases on Oracle server version 12R2 and later. Debezium ingests change events using &lt;a target="_blank" rel="nofollow" href="https://docs.oracle.com/cd/E18283_01/server.112/e16536/logminer.htm"&gt;Oracle’s native LogMiner&lt;/a&gt; database package. Oracle LogMiner is part of the Oracle Database utilities and provides a well-defined, easy-to-use, and comprehensive interface for querying online and archived redo log files.&lt;/p&gt; &lt;p&gt;The first time that the Debezium Oracle connector starts, it performs an initial consistent snapshot of the database to see its entire history. You can change this behavior by setting the &lt;code&gt;snapshot.mode&lt;/code&gt;. After the connector completes its initial snapshot, the Debezium connector continues streaming from the position that it read from the current system change number (SCN) position in the redo log. The initial snapshot ensures that the connector has a complete and consistent set of data.&lt;/p&gt; &lt;p&gt;Consider a customer&amp;#8217;s table defined in the inventory database schema:&lt;/p&gt; &lt;pre&gt;CREATE TABLE customers (   id NUMBER(9) GENERATED BY DEFAULT ON NULL AS IDENTITY (START WITH 1001) NOT NULL PRIMARY KEY,   first_name VARCHAR2(255) NOT NULL,   last_name VARCHAR2(255) NOT NULL,   email VARCHAR2(255) NOT NULL UNIQUE ); &lt;/pre&gt; &lt;p&gt;All data change events produced by the connector have a key and a value, although the key and value structure depends on the source table. The event&amp;#8217;s key has a schema that contains a field for each column in the primary key (or unique key constraint) of the table. The event’s value contains the row’s &lt;code&gt;ID&lt;/code&gt;, &lt;code&gt;FIRST_NAME&lt;/code&gt;, &lt;code&gt;LAST_NAME&lt;/code&gt;, and &lt;code&gt;EMAIL&lt;/code&gt; columns with a representation that depends on the column&amp;#8217;s Oracle &lt;a target="_blank" rel="nofollow" href="https://debezium.io/documentation/reference/connectors/oracle.html#oracle-data-type-mappings"&gt;data type&lt;/a&gt;. For deleted rows, an event provides the consumer with information to process the row&amp;#8217;s removal.&lt;/p&gt; &lt;h2&gt;Setting up your Oracle database&lt;/h2&gt; &lt;p&gt;Using a multi-tenancy configuration with a container database, prepare the database by completing the following steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Prepare the database and replace the size with your expected value; for example, &lt;code&gt;10G&lt;/code&gt;: &lt;pre&gt;ORACLE_SID=ORACLCDB dbz_oracle sqlplus /nolog   CONNECT sys/top_secret AS SYSDBA alter system set db_recovery_file_dest_size = &amp;#60;REPLACE_WITH_YOUR_SIZE&amp;#62;; alter system set db_recovery_file_dest = '/opt/oracle/oradata/recovery_area' scope=spfile; shutdown immediate startup mount alter database archivelog; alter database open; -- Should now "Database log mode: Archive Mode" archive log list   exit; &lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Enable supplemental logging for captured tables, or for the database so that you can capture the &lt;em&gt;before&lt;/em&gt; state of changed database rows: &lt;pre&gt;ALTER DATABASE ADD SUPPLEMENTAL LOG DATA; ALTER TABLE inventory.customers ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS; &lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Set up the user account with the specific permissions so that the connector can capture change events: &lt;pre&gt;sqlplus sys/top_secret@//localhost:1521/ORCLCDB as sysdba   CREATE TABLESPACE logminer_tbs DATAFILE '/opt/oracle/oradata/ORCLCDB/logminer_tbs.dbf'     SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;   exit;   sqlplus sys/top_secret@//localhost:1521/ORCLPDB1 as sysdba   CREATE TABLESPACE logminer_tbs DATAFILE '/opt/oracle/oradata/ORCLCDB/ORCLPDB1/logminer_tbs.dbf'     SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;   exit;   sqlplus sys/top_secret@//localhost:1521/ORCLCDB as sysdba     CREATE USER c##dbzuser IDENTIFIED BY dbz     DEFAULT TABLESPACE logminer_tbs     QUOTA UNLIMITED ON logminer_tbs     CONTAINER=ALL;     GRANT CREATE SESSION TO c##dbzuser CONTAINER=ALL;   GRANT SET CONTAINER TO c##dbzuser CONTAINER=ALL;   GRANT SELECT ON V_$DATABASE to c##dbzuser CONTAINER=ALL;   GRANT FLASHBACK ANY TABLE TO c##dbzuser CONTAINER=ALL;   GRANT SELECT ANY TABLE TO c##dbzuser CONTAINER=ALL;   GRANT SELECT_CATALOG_ROLE TO c##dbzuser CONTAINER=ALL;   GRANT EXECUTE_CATALOG_ROLE TO c##dbzuser CONTAINER=ALL;   GRANT SELECT ANY TRANSACTION TO c##dbzuser CONTAINER=ALL;   GRANT LOGMINING TO c##dbzuser CONTAINER=ALL;     GRANT CREATE TABLE TO c##dbzuser CONTAINER=ALL;   GRANT LOCK ANY TABLE TO c##dbzuser CONTAINER=ALL;   GRANT ALTER ANY TABLE TO c##dbzuser CONTAINER=ALL;   GRANT CREATE SEQUENCE TO c##dbzuser CONTAINER=ALL;     GRANT EXECUTE ON DBMS_LOGMNR TO c##dbzuser CONTAINER=ALL;   GRANT EXECUTE ON DBMS_LOGMNR_D TO c##dbzuser CONTAINER=ALL;     GRANT SELECT ON V_$LOG TO c##dbzuser CONTAINER=ALL;   GRANT SELECT ON V_$LOG_HISTORY TO c##dbzuser CONTAINER=ALL;   GRANT SELECT ON V_$LOGMNR_LOGS TO c##dbzuser CONTAINER=ALL;   GRANT SELECT ON V_$LOGMNR_CONTENTS TO c##dbzuser CONTAINER=ALL;   GRANT SELECT ON V_$LOGMNR_PARAMETERS TO c##dbzuser CONTAINER=ALL;   GRANT SELECT ON V_$LOGFILE TO c##dbzuser CONTAINER=ALL;   GRANT SELECT ON V_$ARCHIVED_LOG TO c##dbzuser CONTAINER=ALL;   GRANT SELECT ON V_$ARCHIVE_DEST_STATUS TO c##dbzuser CONTAINER=ALL;     exit; &lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Deploying the Debezium Oracle connector&lt;/h2&gt; &lt;p&gt;Currently, Red Hat is not shipping the Oracle JDBC driver, but you can download the &lt;a target="_blank" rel="nofollow" href="https://www.oracle.com/database/technologies/instant-client/downloads.html"&gt;Oracle Instant Client&lt;/a&gt; to get it.&lt;/p&gt; &lt;p&gt;The following example shows a JSON request for registering an instance of the Debezium Oracle connector:&lt;/p&gt; &lt;pre&gt;{     "name": "inventory-connector",     "config": {         "connector.class" : "io.debezium.connector.oracle.OracleConnector",         "tasks.max" : "1",         "database.server.name" : "server1",         "database.user" : "c##dbzuser",         "database.password" : "dbz",         "database.url": "jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(LOAD_BALANCE=OFF)(FAILOVER=ON)(ADDRESS=(PROTOCOL=TCP)(HOST=&amp;#60;oracle ip 1&amp;#62;)(PORT=1521))(ADDRESS=(PROTOCOL=TCP)(HOST=&amp;#60;oracle ip 2&amp;#62;)(PORT=1521)))(CONNECT_DATA=SERVICE_NAME=)(SERVER=DEDICATED)))",         "database.dbname" : "ORCLCDB",         "database.pdb.name" : "ORCLPDB1",         "database.history.kafka.bootstrap.servers" : "kafka:9092",         "database.history.kafka.topic": "schema-changes.inventory"     } } &lt;/pre&gt; &lt;p&gt;For more information about the connector properties, see the &lt;a target="_blank" rel="nofollow" href="https://debezium.io/documentation/reference/connectors/oracle.html#oracle-connector-properties"&gt;Debezium documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Get started with Red Hat Integration Debezium connectors for Apache Kafka&lt;/h2&gt; &lt;p&gt;Debezium Apache Kafka connectors are available through Red Hat Integration, which offers a comprehensive set of integration and messaging technologies that connect applications and data across hybrid infrastructures. This agile, distributed, containerized, and API-centric product provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change-data capture, and API management. All combined with a cloud-native platform and toolchain to support the full spectrum of modern application development.&lt;/p&gt; &lt;p&gt;Get started by downloading the Red Hat Integration Debezium CDC connectors from &lt;a href="https://developers.redhat.com/products/amq/download"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fcapture-oracle-database-events-in-apache-kafka-with-debezium%2F&amp;#38;linkname=Capture%20Oracle%20database%20events%20in%20Apache%20Kafka%20with%20Debezium" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fcapture-oracle-database-events-in-apache-kafka-with-debezium%2F&amp;#38;linkname=Capture%20Oracle%20database%20events%20in%20Apache%20Kafka%20with%20Debezium" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fcapture-oracle-database-events-in-apache-kafka-with-debezium%2F&amp;#38;linkname=Capture%20Oracle%20database%20events%20in%20Apache%20Kafka%20with%20Debezium" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fcapture-oracle-database-events-in-apache-kafka-with-debezium%2F&amp;#38;linkname=Capture%20Oracle%20database%20events%20in%20Apache%20Kafka%20with%20Debezium" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fcapture-oracle-database-events-in-apache-kafka-with-debezium%2F&amp;#38;linkname=Capture%20Oracle%20database%20events%20in%20Apache%20Kafka%20with%20Debezium" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fcapture-oracle-database-events-in-apache-kafka-with-debezium%2F&amp;#38;linkname=Capture%20Oracle%20database%20events%20in%20Apache%20Kafka%20with%20Debezium" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fcapture-oracle-database-events-in-apache-kafka-with-debezium%2F&amp;#38;linkname=Capture%20Oracle%20database%20events%20in%20Apache%20Kafka%20with%20Debezium" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fcapture-oracle-database-events-in-apache-kafka-with-debezium%2F&amp;#038;title=Capture%20Oracle%20database%20events%20in%20Apache%20Kafka%20with%20Debezium" data-a2a-url="https://developers.redhat.com/blog/2021/04/19/capture-oracle-database-events-in-apache-kafka-with-debezium/" data-a2a-title="Capture Oracle database events in Apache Kafka with Debezium"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/19/capture-oracle-database-events-in-apache-kafka-with-debezium/"&gt;Capture Oracle database events in Apache Kafka with Debezium&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rcq4W10L08g" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;One of the most requested connector plug-ins is coming to Red Hat Integration. You can now stream your data from Oracle databases with the Debezium connector for Oracle in developer preview. With this new connector, developers can leverage the power of the 100% open source Debezium project to stream their Oracle data to Red Hat [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/19/capture-oracle-database-events-in-apache-kafka-with-debezium/"&gt;Capture Oracle database events in Apache Kafka with Debezium&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/19/capture-oracle-database-events-in-apache-kafka-with-debezium/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">885787</post-id><dc:creator>Hugo Guerrero</dc:creator><dc:date>2021-04-19T07:00:22Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/19/capture-oracle-database-events-in-apache-kafka-with-debezium/</feedburner:origLink></entry><entry><title>Connect AMQ Streams to your Red Hat OpenShift 4 monitoring stack</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/5vkOQe_p-pY/" /><category term="Containers" /><category term="Developer Tools" /><category term="Java" /><category term="Kubernetes" /><category term="amq streams" /><category term="grafana dashboards" /><category term="OpenShift monitoring" /><category term="Strimzi" /><author><name>David Kornel</name></author><id>https://developers.redhat.com/blog/?p=786757</id><updated>2021-04-19T07:00:10Z</updated><published>2021-04-19T07:00:10Z</published><content type="html">&lt;p&gt;Monitoring systems in use is one of the greatest challenges in &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes"&gt;cloud environments&lt;/a&gt;. Users always want to know how their applications work in production. For example, they want to know how &lt;a target="_blank" rel="nofollow" href="products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; utilizes its resources; or how to monitor systems in use like &lt;a target="_blank" rel="nofollow" href="/products/amq/download"&gt;Red Hat AMQ Streams&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;AMQ Streams, the enterprise version of &lt;a target="_blank" rel="nofollow" href="https://strimzi.io/"&gt;Strimzi&lt;/a&gt;, exports many useful metrics from &lt;a target="_blank" rel="nofollow" href="/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; clusters, Apache Zookeeper clusters, and other components. We can use &lt;a target="_blank" rel="nofollow" href="https://prometheus.io"&gt;Prometheus&lt;/a&gt; to scrape these metrics and display them in &lt;a target="_blank" rel="nofollow" href="https://grafana.com"&gt;Grafana&lt;/a&gt; dashboards. Exporting AMQ Streams metrics to Grafana is quite easy, and using the existing monitoring stack on OpenShift 4 is easy, as well.&lt;/p&gt; &lt;p&gt;This article shows you how to quickly set up a new or pre-existing AMQ Streams deployment with a default &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.7/monitoring/configuring-the-monitoring-stack.html"&gt;OpenShift 4 monitoring stack&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Install AMQ Streams and Grafana&lt;/h2&gt; &lt;p&gt;All users can use an existing monitoring stack in one of two ways: Create a new namespace and deploy AMQ Streams from scratch or use a pre-existing AMQ Streams instance and update the configuration namespaces where it operates.&lt;/p&gt; &lt;p&gt;Either way, the following example assumes you have cluster-wide AMQ Streams &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/operators"&gt;Operator&lt;/a&gt; in one namespace and Kafka clusters in different namespaces. We will not create a new namespace; instead, we&amp;#8217;ll use &lt;code&gt;streams-cluster-operator&lt;/code&gt; for the AMQ Streams Operator and &lt;code&gt;streams-Kafka-cluster&lt;/code&gt; for the Kafka cluster, and switch between them.&lt;/p&gt; &lt;p style="padding-left: 40px"&gt;&lt;strong&gt;Note&lt;/strong&gt;: See &lt;a href="https://developers.redhat.com/products/amq/hello-world-amq-streams-openshift#fndtn-macos"&gt;&lt;i&gt;Hello World for AMQ Streams on OpenShift&lt;/i&gt;&lt;/a&gt; for a detailed guide to installing AMQ Streams from scratch. You can follow all the steps there or check the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/2020.q4/html/deploying_and_upgrading_amq_streams_on_openshift/deploy-tasks_str#cluster-operator-str"&gt;Red Hat AMQ Streams documentation&lt;/a&gt;, which describes the same steps.&lt;/p&gt; &lt;h3&gt;Confirm your new AMQ Streams installation&lt;/h3&gt; &lt;p&gt;Once you are done setting up your AMQ Streams installation, you should see the &lt;code&gt;strimzi-cluster-operator&lt;/code&gt; pod up and running:&lt;/p&gt; &lt;pre&gt;$ oc get pod -n streams-cluster-operator strimzi-cluster-operator-7bff5b4d65-jmgqr    1/1     Running   0          12m &lt;/pre&gt; &lt;h3&gt;Deploy a Kafka cluster&lt;/h3&gt; &lt;p&gt;Next, we&amp;#8217;ll deploy a Kafka cluster with the metrics configured. During the AMQ Streams installation, you downloaded examples with installed files for Operators, Kafka, and metrics. An example of a &lt;code&gt;Kafka&lt;/code&gt; custom resource with metrics was stored under &lt;code&gt;examples/metrics/Kafka-metrics.yaml&lt;/code&gt;. The &lt;code&gt;Kafka&lt;/code&gt; metrics are now stored in a new config map, which is referenced in the &lt;code&gt;Kafka&lt;/code&gt; custom resource. The metrics format hasn&amp;#8217;t changed. Anytime that you change the metrics configuration, you will also need to change the config map.&lt;/p&gt; &lt;p&gt;As an example, you can use the AMQ Streams default configuration and deploy everything by executing the following command:&lt;/p&gt; &lt;pre&gt;$ oc apply -f examples/metrics/kafka-metrics.yaml -n streams-kafka-cluster&lt;/pre&gt; &lt;p&gt;After a time, you should see the Kafka cluster up and running:&lt;/p&gt; &lt;pre&gt;$ oc get pod -n streams-kafka-cluster my-cluster-entity-operator-cf887b59-645zt    3/3     Running   0          6m my-cluster-kafka-0                           1/1     Running   0          9m my-cluster-kafka-1                           1/1     Running   0          9m my-cluster-kafka-2                           1/1     Running   0          9m my-cluster-kafka-exported-fsf343r     1/1     Running   0          5m my-cluster-zookeeper-0                       1/1     Running   0          11m my-cluster-zookeeper-1                       1/1     Running   0          11m my-cluster-zookeeper-2                       1/1     Running   0          11m &lt;/pre&gt; &lt;p&gt;You can change the name in the custom resource and deploy other clusters in different namespaces. As a result of the cluster-wide installation, the AMQ Streams Operator will serve all clusters deployed into an OpenShift cluster.&lt;/p&gt; &lt;h3&gt;Install the Grafana Operator&lt;/h3&gt; &lt;p&gt;In this section, we&amp;#8217;ll set up a Granfana instance. Grafana is by default installed in every OpenShift 4 instance. Unfortunately, the pre-installed Grafana instance is read-only, and you can only use predefined Grafana dashboards. As a result, we are forced to deploy our own Grafana instance into OpenShift.&lt;/p&gt; &lt;p&gt;Let’s start with a new namespace called &lt;code&gt;streams-grafana&lt;/code&gt;. Then, we&amp;#8217;ll install Grafana from the OpenShift OperatorHub by creating proper &lt;code&gt;operatorgroup&lt;/code&gt; and &lt;code&gt;subscription&lt;/code&gt;. Here is the process to install the Grafana Operator:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Make a new namespace: &lt;pre&gt;$ oc create namespace streams-grafana&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Create an Operator group: &lt;pre&gt;$ cat &amp;#60;&amp;#60; EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata:   name: grafana-group   namespace: streams-grafana   labels:     app: grafana spec:   targetNamespaces:     - streams-grafana EOF&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Create a subscription for the Grafana Operator: &lt;pre&gt;$ cat &amp;#60;&amp;#60; EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata:   name: grafana-operator   namespace:  streams-grafana spec:   channel: alpha   installPlanApproval: Automatic   name: grafana-operator   source: community-operators   sourceNamespace: openshift-marketplace   startingCSV: grafana-operator.v3.9.0 EOF&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Confirm the Grafana Operator is installed successfully: &lt;pre&gt;$ oc get pods -n streams-grafana NAME                               READY   STATUS    RESTARTS   AGE grafana-operator-957c6dcd9-wrljw   1/1     Running   0          65s &lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Connect the AMQ Streams Operator and Kafka clusters to your monitoring stack&lt;/h2&gt; &lt;p&gt;During the OpenShift installation, the default OpenShift 4 monitoring stack is deployed in the &lt;code&gt;openshift-monitoring&lt;/code&gt; namespace. With additional configuration, you can re-use it for monitoring your application.&lt;/p&gt; &lt;p&gt;First, you need to allow &lt;code&gt;user-workloads&lt;/code&gt; in your OpenShift cluster. You could achieve this by creating a new config map to the &lt;code&gt;openshift-monitoring&lt;/code&gt; namespace. Here is the config map for our example:&lt;/p&gt; &lt;pre&gt;$ cat &amp;#60;&amp;#60; EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: | enableUserWorkload: true EOF &lt;/pre&gt; &lt;p&gt;After applying the config map, you should see new pods in the &lt;code&gt;openshift-user-workload-monitoring&lt;/code&gt; namespace:&lt;/p&gt; &lt;pre&gt;$ oc get po -n openshift-user-workload-monitoring NAME                                   READY   STATUS    RESTARTS   AGE prometheus-operator-868cd68496-jl44r   2/2     Running   0          118s prometheus-user-workload-0             5/5     Running   1          112s prometheus-user-workload-1             5/5     Running   1          112s thanos-ruler-user-workload-0           3/3     Running   0          111s thanos-ruler-user-workload-1           3/3     Running   0          111s &lt;/pre&gt; &lt;p&gt;Second, you have to deploy pod monitors for all AMQ Streams components. You can find the pod monitors YAML in &lt;code&gt;examples/metrics/prometheus-install/strimzi-pod-monitor.yaml&lt;/code&gt;. You need to create a pod monitor for each namespace and component you use. In general, you need to have a pod monitor for &lt;code&gt;Kafka&lt;/code&gt; in the &lt;code&gt;kafka&lt;/code&gt; namespace, one for for &lt;code&gt;cluster-operator&lt;/code&gt; in the &lt;code&gt;cluster-operator&lt;/code&gt; namespace, and so on:&lt;/p&gt; &lt;pre&gt;$ cat examples/metrics/prometheus-install/strimzi-pod-monitor.yaml | sed "s#myproject#streams-kafka-cluster#g" | oc apply -n streams-kafka-cluster -f - $ cat examples/metrics/prometheus-install/strimzi-pod-monitor.yaml | sed "s#myproject#streams-cluster-operator#g" | oc apply -n streams-cluster-operator -f - &lt;/pre&gt; &lt;p&gt;Note that you can remove unused pod monitors, such as &lt;code&gt;KafkaBridge&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Check Prometheus for Strimzi or Kafka queries&lt;/h3&gt; &lt;p&gt;Finally, in the OpenShift administrator console, navigate to the &lt;b&gt;Monitoring&lt;/b&gt; tab and open &lt;b&gt;Metrics&lt;/b&gt;. Try a few Strimzi or Kafka-related queries. Figure 1 shows a query for &lt;code&gt;strimzi_resources&lt;/code&gt;.&lt;/p&gt; &lt;div id="attachment_789517" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-24-at-15.27.50.png"&gt;&lt;img aria-describedby="caption-attachment-789517" class="wp-image-789517 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-24-at-15.27.50-1024x590.png" alt="Querying Prometheus in the OpenShift console." width="640" height="369" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-24-at-15.27.50-1024x590.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-24-at-15.27.50-300x173.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-24-at-15.27.50-768x443.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-789517" class="wp-caption-text"&gt;Figure 1: Querying Prometheus in the OpenShift administrator console.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Connect a Granfana instance to OpenShift 4 Prometheus&lt;/h2&gt; &lt;p&gt;Now, we are ready to create a Grafana instance. Because the Grafana Operator is up and running, we just need to deploy the Grafana instance using the following commands:&lt;/p&gt; &lt;pre&gt;$ cat &amp;#60;&amp;#60; EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: Grafana metadata:   name: grafana   namespace: streams-grafana spec:   ingress:     enabled: True   config:     log:       mode: "console"       level: "warn"     security:       admin_user: "admin"       admin_password: "admin"     auth:       disable_login_form: False       disable_signout_menu: True     auth.anonymous:       enabled: True   dashboardLabelSelector:     - matchExpressions:         - { key: app, operator: In, values: [strimzi] }   resources:     limits:       cpu: 2000m       memory: 8000Mi     requests:       cpu: 100m       memory: 200Mi EOF&lt;/pre&gt; &lt;p&gt;With the Grafana instance running, we need to create a data source from Prometheus. Before we can do that, we need to create a &lt;code&gt;ServiceAccount&lt;/code&gt; and &lt;code&gt;ClusterRoleBinding&lt;/code&gt; for Grafana. Here is the &lt;code&gt;ServiceAccount&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ cat &amp;#60;&amp;#60; EOF | oc apply -f - apiVersion: v1 kind: ServiceAccount metadata: name: grafana-serviceaccount labels: app: strimzi namespace: streams-grafana EOF &lt;/pre&gt; &lt;p&gt;And here is the &lt;code&gt;ClusterRoleBinding&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ cat &amp;#60;&amp;#60; EOF | oc apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: grafana-cluster-monitoring-binding labels: app: strimzi subjects: - kind: ServiceAccount name: grafana-serviceaccount namespace: streams-monitoring roleRef: kind: ClusterRole name: cluster-monitoring-view apiGroup: rbac.authorization.k8s.io EOF&lt;/pre&gt; &lt;p&gt;Now, we can get a token to grant access for Grafana into Prometheus:&lt;/p&gt; &lt;pre&gt;$ export TOKEN=$(oc serviceaccounts get-token grafana-serviceaccount -n streams-grafana)&lt;/pre&gt; &lt;p&gt;When you have the token, run the following command to create a data source and pass the token to it:&lt;/p&gt; &lt;pre&gt;$ cat &amp;#60;&amp;#60; EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: GrafanaDataSource metadata:   name: grafanadatasource   namespace: streams-grafana spec:   name: middleware.yaml   datasources:     - name: Prometheus       type: prometheus       access: proxy       url: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091       basicAuth: true       basicAuthPassword: $TOKEN       basicAuthUser: internal       isDefault: true       version: 1       editable: true       jsonData:         tlsSkipVerify: true         timeInterval: "5s" EOF &lt;/pre&gt; &lt;h2&gt;Set up your AMQ Streams dashboard in the Grafana instance&lt;/h2&gt; &lt;p&gt;Example dashboards for AMQ Streams are available in the &lt;code&gt;examples/metrics/grafana-dashboards&lt;/code&gt; folder. There are two ways to include a dashboard in a Grafana instance. One option is to use the Grafana user interface (UI) and navigate to the &lt;strong&gt;Dashboards&lt;/strong&gt; page. You can include available dashboards and select a proper data source. The second option is to create a &lt;code&gt;GrafanaDashboard&lt;/code&gt; custom resource with the dashboard JSON inside it.&lt;/p&gt; &lt;p&gt;For this example, we&amp;#8217;ll take the second option. Below is a script that gets the JSON from the examples folder and makes a collection runner. Go to the Grafana dashboards that you want to use and replace the &lt;code&gt;.spec.name&lt;/code&gt; of your JSON dashboard and the &lt;code&gt;.metadata.name&lt;/code&gt; of your custom resource (CR) definition, as shown here:&lt;/p&gt; &lt;pre&gt;$ cat &amp;#60;&amp;#60; EOF &amp;#62; /tmp/dashboard.yaml apiVersion: integreatly.org/v1alpha1 kind: GrafanaDashboard metadata: labels: app: strimzi monitoring-key: middleware name: strimzi-operators namespace: streams-grafana spec: name: strimzi-operators.json json: | PLACEHOLDER EOF $ DATA="$(jq 'del(.__inputs,.__requires)' examples/metrics/grafana-dashboards/strimzi-operators.json)" yq eval ".spec.json = strenv(DATA)" /tmp/dashboard.yaml | sed -e '/DS_PROMETHEUS/d' | oc apply -f -&lt;/pre&gt; &lt;p&gt;You can use the same commands for other dashboards, as well; just keep in mind that you have to change &lt;code&gt;.spec.name&lt;/code&gt; and &lt;code&gt;.metadata.name&lt;/code&gt; if you do that.&lt;/p&gt; &lt;p&gt;Finally, you can access the Grafana UI via the exported route:&lt;/p&gt; &lt;pre&gt;$ oc get route grafana-route -n streams-grafana -o=jsonpath='{.spec.host}'&lt;/pre&gt; &lt;p&gt;Once you are in the UI, you can see your imported dashboards. Figure 2 shows how the AMQ Streams example dashboards represent metrics from the installed Operators.&lt;/p&gt; &lt;div id="attachment_889307" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.54.png"&gt;&lt;img aria-describedby="caption-attachment-889307" class="wp-image-889307 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.54-1024x592.png" alt="Metrics from the AMQ Streams Operators." width="640" height="370" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.54-1024x592.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.54-300x173.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.54-768x444.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889307" class="wp-caption-text"&gt;Figure 2: Metrics from the AMQ Streams Operators.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 3 shows Kafka metrics inside the example dashboard.&lt;/p&gt; &lt;div id="attachment_889297" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.46.png"&gt;&lt;img aria-describedby="caption-attachment-889297" class="wp-image-889297 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.46-1024x592.png" alt="A Kafka dashboard with collected data." width="640" height="370" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.46-1024x592.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.46-300x173.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.46-768x444.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889297" class="wp-caption-text"&gt;Figure 3: A Kafka dashboard with collected data.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Finally, Figure 4 shows the Kafka Exporter dashboard. This dashboard shows various topics and the clients attached to them.&lt;/p&gt; &lt;div id="attachment_889287" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.36.png"&gt;&lt;img aria-describedby="caption-attachment-889287" class="wp-image-889287 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.36-1024x592.png" alt="The Kafka Exporter dashboard with collected data." width="640" height="370" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.36-1024x592.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.36-300x173.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-31-at-16.50.36-768x444.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889287" class="wp-caption-text"&gt;Figure 4: The Kafka Exporter dashboard with collected data.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Set up your AMQ Streams dashboard in OpenShift 4&lt;/h2&gt; &lt;p&gt;It is also possible to use OpenShift dashboards without installing Grafana. The OpenShift dashboards are limited compared to Grafana, but they&amp;#8217;re enough for a quick overview. You will find the OpenShift dashboards in the admin console under &lt;b&gt;Monitoring&lt;/b&gt;. As an example, you could use the following commands to import a Strimzi dashboard:&lt;/p&gt; &lt;pre&gt;$ cat &amp;#60;&amp;#60; EOF &amp;#62; /tmp/file.yaml kind: ConfigMap apiVersion: v1 metadata: name: strimzi-operators-dashboard namespace: openshift-config-managed labels: console.openshift.io/dashboard: 'true' data: strimzi-operators: |- PLACEHOLDER EOF $ DATA="$(jq 'del(.__inputs,.__requires)'  strimzi-examples/examples/metrics/grafana-dashboards/strimzi-operators.json)" yq eval ".data.strimzi-operators = env(DATA)" /tmp/file.yaml | sed -e 's/DS_PROMETHEUS/d' | oc apply -f - &lt;/pre&gt; &lt;p&gt;Now, you should be able to list the &lt;code&gt;strimzi-operators&lt;/code&gt; dashboard in the OpenShift UI and see something similar to what&amp;#8217;s shown in Figure 5.&lt;/p&gt; &lt;div id="attachment_888987" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Screen-Shot-2021-03-31-at-10.17.48.png"&gt;&lt;img aria-describedby="caption-attachment-888987" class="wp-image-888987 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Screen-Shot-2021-03-31-at-10.17.48-1024x592.png" alt="Kafka metrics displayed in the Openshift 4 dashboard." width="640" height="370" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Screen-Shot-2021-03-31-at-10.17.48-1024x592.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/Screen-Shot-2021-03-31-at-10.17.48-300x173.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/Screen-Shot-2021-03-31-at-10.17.48-768x444.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-888987" class="wp-caption-text"&gt;Figure 5: An AMQ Streams dashboard in the OpenShift 4 dashboards view.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, you&amp;#8217;ve seen how to set up and use the OpenShift 4 monitoring stack to monitor AMQ Streams. You can monitor existing AMQ Streams clusters without needing to redeploy AMQ Streams, and you can also set up your own Grafana dashboard for more detailed metrics. Everything you need to apply proper custom resources is available from the &lt;code&gt;oc&lt;/code&gt; client. If you want to avoid manual copying and pasting, you can use the &lt;code&gt;yq&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fconnect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack%2F&amp;#38;linkname=Connect%20AMQ%20Streams%20to%20your%20Red%20Hat%20OpenShift%204%20monitoring%20stack" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fconnect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack%2F&amp;#38;linkname=Connect%20AMQ%20Streams%20to%20your%20Red%20Hat%20OpenShift%204%20monitoring%20stack" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fconnect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack%2F&amp;#38;linkname=Connect%20AMQ%20Streams%20to%20your%20Red%20Hat%20OpenShift%204%20monitoring%20stack" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fconnect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack%2F&amp;#38;linkname=Connect%20AMQ%20Streams%20to%20your%20Red%20Hat%20OpenShift%204%20monitoring%20stack" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fconnect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack%2F&amp;#38;linkname=Connect%20AMQ%20Streams%20to%20your%20Red%20Hat%20OpenShift%204%20monitoring%20stack" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fconnect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack%2F&amp;#38;linkname=Connect%20AMQ%20Streams%20to%20your%20Red%20Hat%20OpenShift%204%20monitoring%20stack" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fconnect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack%2F&amp;#38;linkname=Connect%20AMQ%20Streams%20to%20your%20Red%20Hat%20OpenShift%204%20monitoring%20stack" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F19%2Fconnect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack%2F&amp;#038;title=Connect%20AMQ%20Streams%20to%20your%20Red%20Hat%20OpenShift%204%20monitoring%20stack" data-a2a-url="https://developers.redhat.com/blog/2021/04/19/connect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack/" data-a2a-title="Connect AMQ Streams to your Red Hat OpenShift 4 monitoring stack"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/19/connect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack/"&gt;Connect AMQ Streams to your Red Hat OpenShift 4 monitoring stack&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/5vkOQe_p-pY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Monitoring systems in use is one of the greatest challenges in cloud environments. Users always want to know how their applications work in production. For example, they want to know how Red Hat OpenShift utilizes its resources; or how to monitor systems in use like Red Hat AMQ Streams. AMQ Streams, the enterprise version of [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/19/connect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack/"&gt;Connect AMQ Streams to your Red Hat OpenShift 4 monitoring stack&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/19/connect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">786757</post-id><dc:creator>David Kornel</dc:creator><dc:date>2021-04-19T07:00:10Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/19/connect-amq-streams-to-your-red-hat-openshift-4-monitoring-stack/</feedburner:origLink></entry><entry><title type="html">Kogito Serverless Workflow at OpenShift Commons Briefing 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_EiM8cgQ3jk/kogito-serverless-workflow-at-openshift-commons-briefing-2021.html" /><author><name>Ricardo Zanini</name></author><id>https://blog.kie.org/2021/04/kogito-serverless-workflow-at-openshift-commons-briefing-2021.html</id><updated>2021-04-16T21:17:46Z</updated><content type="html">Following Monday (April 19th), we will have a presentation during the . In this talk, we will briefly introduce the CNCF Serverless Workflow Specification project and talk about the Kogito implementation and Knative Eventing integration. We prepared a special demonstration just for this talk available at our repository later next week. In this demo, we will run through a use case to understand how to use Kogito Serverless Workflow to orchestrate CloudEvents on top of Knative Eventing. Come to the talk, and let’s have some enjoyable time together learning about events and workflows! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_EiM8cgQ3jk" height="1" width="1" alt=""/&gt;</content><dc:creator>Ricardo Zanini</dc:creator><feedburner:origLink>https://blog.kie.org/2021/04/kogito-serverless-workflow-at-openshift-commons-briefing-2021.html</feedburner:origLink></entry><entry><title>Broadening compiler checks for buffer overflows in _FORTIFY_SOURCE</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/2n6K5hNpQlw/" /><category term="C" /><category term="Linux" /><category term="Security" /><category term="_FORTIFY_SOURCE" /><category term="buffer overflow" /><category term="clang" /><category term="glibc" /><author><name>Siddhesh Poyarekar</name></author><id>https://developers.redhat.com/blog/?p=854517</id><updated>2021-04-16T07:00:56Z</updated><published>2021-04-16T07:00:56Z</published><content type="html">&lt;p&gt;Buffer overruns are by far the most common vulnerability in &lt;a target="_blank" rel="nofollow" href="/topics/c/"&gt;C or C++&lt;/a&gt; programs, and a number of techniques have come up over the years to detect overruns early and abort execution. The &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; macro, provided by the GNU C Library, helps mitigate a number of these overruns and is widely deployed in &lt;a target="_blank" rel="nofollow" href="/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt;. &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/blogs/766093/posts/1976213"&gt;This article on the Red Hat Security blog&lt;/a&gt; is a good introduction to &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;In the GNU C Library&amp;#8217;s 2.33 release, we added a new level for &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; to improve the protections the macro provides. Here we take a closer look at the internals of &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; in GCC and explore the need for this new level.&lt;/p&gt; &lt;h2&gt;_FORTIFY_SOURCE under the hood&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; macro replaces some common functions with fortified wrappers that check whether the size of the destination buffer in those functions is large enough to handle the input data. If the check fails, the program aborts, and the users are saved from what could have been an exploitable buffer overflow. The actual implementation of a fortified wrapper function has a lot of glibc-isms, but it is conceptually a wrapper for &lt;code&gt;wmemcpy&lt;/code&gt; that looks like the following:&lt;/p&gt; &lt;pre&gt;extern inline wchar_t * __attribute__ ((always_inline)) wmemcpy (wchar_t *__restrict __s1, const wchar_t *__restrict __s2, size_t __n) { if (__builtin_object_size (__s1, 0) != (size_t) -1) return __wmemcpy_chk (__s1, __s2, __n, __builtin_object_size (__s1, 0) / sizeof (wchar_t)); return __original_wmemcpy (__s1, __s2, __n); } &lt;/pre&gt; &lt;p&gt;The GNU C Library implements similar fortified wrappers for a number of functions. Some wrappers look slightly different; for example, the &lt;code&gt;memcpy&lt;/code&gt; wrapper unconditionally calls a builtin named &lt;code&gt;__builtin___memcpy_chk&lt;/code&gt;. Those built-ins also evaluate to roughly the wrapper function just shown.&lt;/p&gt; &lt;h3&gt;The cornerstone: &lt;code&gt;__builtin_object_size&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;As the sample source code shows, the core functionality of &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; is based around the &lt;code&gt;__builtin_object_size&lt;/code&gt; builtin. This builtin evaluates objects that the first argument (which is a pointer) points to and returns an estimate for the size of the object. Based on the second argument to the builtin, the size may be a minimum estimate or a maximum estimate. If the builtin fails to deduce the object size, then it either returns &lt;code&gt;(size_t)0&lt;/code&gt; or &lt;code&gt;(size_t)-1&lt;/code&gt; depending on whether the minimum or maximum estimate is requested.&lt;/p&gt; &lt;p&gt;The compiler translates the wrapper to either a call to &lt;code&gt;__wmmcpy_chk&lt;/code&gt; or to the original &lt;code&gt;wmemcpy&lt;/code&gt;, denoted by &lt;code&gt;__original_wmemcpy&lt;/code&gt; in the example above. Most importantly, the compiler optimizes away the &lt;code&gt;__builtin_object_size&lt;/code&gt; comparison because it is a constant expression: &lt;code&gt;__builtin_object_size&lt;/code&gt; always returns a constant. If &lt;code&gt;__builtin_object_size&lt;/code&gt; is unable to deduce a constant object size, the compiler falls back to the default implementation &lt;code&gt;wmemcpy&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Validate before you leap&lt;/h3&gt; &lt;p&gt;The checking variant of &lt;code&gt;wmemcpy&lt;/code&gt;—i.e., &lt;code&gt;__wmemcpy_chk&lt;/code&gt;—is trivially implemented as follows:&lt;/p&gt; &lt;pre&gt;wchar_t * __wmemcpy_chk (wchar_t *s1, const wchar_t *s2, size_t n, size_t ns1) { if (__glibc_unlikely (ns1 &amp;#60; n)) __chk_fail (); return (wchar_t *) memcpy ((char *) s1, (char *) s2, n * sizeof (wchar_t)); } &lt;/pre&gt; &lt;p&gt;where &lt;code&gt;__chk_fail&lt;/code&gt; aborts the program. At first glance, this looks slow because it adds a condition to every call. However, &lt;code&gt;glibc&lt;/code&gt; lays out the performance-sensitive functions in a way that the overhead at runtime is constant, and in practice negligible.&lt;/p&gt; &lt;h2&gt;In the very long term, everything is dynamic&lt;/h2&gt; &lt;p&gt;The obvious extension to this functionality is to allow &lt;code&gt;__builtin_object_size&lt;/code&gt; to return non-constant results. That idea eventually took the form of a new builtin called &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt;, implemented in LLVM 9. This builtin intends to be a drop-in replacement for &lt;code&gt;__builtin_object_size&lt;/code&gt;, although there some differences. Like &lt;code&gt;__builtin_object_size&lt;/code&gt;, the new builtin attempts to evaluate the size of the object to a constant. If the object size is not constant, the builtin tries to deduce an expression that evaluates the size of the object. If even that is not possible, it bails out and returns &lt;code&gt;(size_t)-1&lt;/code&gt; or &lt;code&gt;(size_t)0&lt;/code&gt;. Overall, this allows more cases where the size of an object can be known well enough to do something useful with it at runtime.&lt;/p&gt; &lt;h3&gt;A third fortification level&lt;/h3&gt; &lt;p&gt;With glibc 2.33, this support materialized into an actual hardening feature in the form of a new fortification level: &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt;. This fortification level widens coverage of cases that &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; can catch. For example, the following function:&lt;/p&gt; &lt;pre&gt;size_t add_size; size_t multiplier; void * do_something (void *in, size_t insz, size_t sz) { void *buf = malloc ((sz + add_size) * multiplier); memcpy (buf, in, insz); return buf; } &lt;/pre&gt; &lt;p&gt;when built with &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt;, does not result in a call to the fortified version of &lt;code&gt;memcpy&lt;/code&gt; (i.e., &lt;code&gt;__memcpy_chk&lt;/code&gt;). As a result, cases where &lt;code&gt;insz&lt;/code&gt; may be bigger than the size of &lt;code&gt;buf&lt;/code&gt; slip through.&lt;/p&gt; &lt;p&gt;With &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt;, though, &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt; can evaluate to &lt;code&gt;(sz + add_size) * multiplier&lt;/code&gt;. With this expression, the compiler can generate a call to &lt;code&gt;__memcpy_chk&lt;/code&gt; and allow fortification. This promises to significantly widen fortification coverage to include cases where the compiler can see the non-constant expression for object size. This is a great improvement, but we decided against implementing this at &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; level &lt;code&gt;2&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Runtime overhead in &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;Earlier &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; levels rely on constant object sizes; because of this, the runtime overhead is negligible. &lt;code&gt;_FORTIFY_SOURCE=3,&lt;/code&gt; however, changes that because expressions used to compute the object size can be arbitrarily complex. Complex expressions can add arbitrarily more runtime overhead. Further, consider the possibility of &lt;code&gt;do_something&lt;/code&gt; in the previous example being called in a loop; the overhead gets magnified. This was a good enough reason to not sneak this new functionality in under the hood. The new level lets developers tinker around with it and decide whether the overhead was acceptable for their use case.&lt;/p&gt; &lt;h2&gt;What&amp;#8217;s next for _FORTIFY_SOURCE&lt;/h2&gt; &lt;p&gt;GCC support for &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt; or equivalent functionality is in progress. At the moment this is available only when building applications with LLVM. There are some unspecified corner cases with &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt; that may result in avoidable performance overheads. We hope to iron those out with the GCC implementation and feed it back into LLVM, thus making both implementations consistent and performant.&lt;/p&gt; &lt;p&gt;&lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; begins a shift in design for source fortification as we accept variable performance overheads for the checking. This also adds potential for additional coverage in the future as compilers get smarter at deducing object sizes. This is hopefully just the beginning!&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fbroadening-compiler-checks-for-buffer-overflows-in-_fortify_source%2F&amp;#38;linkname=Broadening%20compiler%20checks%20for%20buffer%20overflows%20in%20_FORTIFY_SOURCE" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fbroadening-compiler-checks-for-buffer-overflows-in-_fortify_source%2F&amp;#38;linkname=Broadening%20compiler%20checks%20for%20buffer%20overflows%20in%20_FORTIFY_SOURCE" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fbroadening-compiler-checks-for-buffer-overflows-in-_fortify_source%2F&amp;#38;linkname=Broadening%20compiler%20checks%20for%20buffer%20overflows%20in%20_FORTIFY_SOURCE" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fbroadening-compiler-checks-for-buffer-overflows-in-_fortify_source%2F&amp;#38;linkname=Broadening%20compiler%20checks%20for%20buffer%20overflows%20in%20_FORTIFY_SOURCE" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fbroadening-compiler-checks-for-buffer-overflows-in-_fortify_source%2F&amp;#38;linkname=Broadening%20compiler%20checks%20for%20buffer%20overflows%20in%20_FORTIFY_SOURCE" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fbroadening-compiler-checks-for-buffer-overflows-in-_fortify_source%2F&amp;#38;linkname=Broadening%20compiler%20checks%20for%20buffer%20overflows%20in%20_FORTIFY_SOURCE" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fbroadening-compiler-checks-for-buffer-overflows-in-_fortify_source%2F&amp;#38;linkname=Broadening%20compiler%20checks%20for%20buffer%20overflows%20in%20_FORTIFY_SOURCE" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fbroadening-compiler-checks-for-buffer-overflows-in-_fortify_source%2F&amp;#038;title=Broadening%20compiler%20checks%20for%20buffer%20overflows%20in%20_FORTIFY_SOURCE" data-a2a-url="https://developers.redhat.com/blog/2021/04/16/broadening-compiler-checks-for-buffer-overflows-in-_fortify_source/" data-a2a-title="Broadening compiler checks for buffer overflows in _FORTIFY_SOURCE"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/16/broadening-compiler-checks-for-buffer-overflows-in-_fortify_source/"&gt;Broadening compiler checks for buffer overflows in _FORTIFY_SOURCE&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/2n6K5hNpQlw" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Buffer overruns are by far the most common vulnerability in C or C++ programs, and a number of techniques have come up over the years to detect overruns early and abort execution. The _FORTIFY_SOURCE macro, provided by the GNU C Library, helps mitigate a number of these overruns and is widely deployed in Red Hat [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/16/broadening-compiler-checks-for-buffer-overflows-in-_fortify_source/"&gt;Broadening compiler checks for buffer overflows in _FORTIFY_SOURCE&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/16/broadening-compiler-checks-for-buffer-overflows-in-_fortify_source/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">854517</post-id><dc:creator>Siddhesh Poyarekar</dc:creator><dc:date>2021-04-16T07:00:56Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/16/broadening-compiler-checks-for-buffer-overflows-in-_fortify_source/</feedburner:origLink></entry><entry><title>Using the SystemTap Dyninst runtime environment</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/gVbvMRdmxd8/" /><category term="C" /><category term="Linux" /><category term="Python" /><category term="dyninst" /><category term="event loop" /><category term="ptrace" /><category term="systemtap" /><category term="user space" /><author><name>Stan Cox</name></author><id>https://developers.redhat.com/blog/?p=828147</id><updated>2021-04-16T07:00:23Z</updated><published>2021-04-16T07:00:23Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/systemtap_beginners_guide/understanding-how-systemtap-works"&gt;SystemTap (stap)&lt;/a&gt; uses a command-line interface (CLI) and a scripting language to write instrumentation for a live running kernel or a user space application. A SystemTap script associates handlers with named events. This means, when a specified event occurs, the default SystemTap kernel runtime runs the handler in the kernel as if it is a quick subroutine, and then it resumes.&lt;/p&gt; &lt;p&gt;SystemTap translates the script to C, uses it to create a kernel module, loads the module, and connects the probed events. It can set probes at arbitrary kernel locations or at user space locations. While SystemTap is a powerful tool, loading the kernel module requires privilege, and this privilege can sometimes be a barrier for use. For example, on managed machines or in containers that are without the necessary privilege. In these cases, SystemTap has another runtime that uses the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_developer_toolset/7/html/user_guide/chap-dyninst"&gt;Dyninst&lt;/a&gt; instrumentation framework to provide many features of the kernel module runtime only requiring user privilege.&lt;/p&gt; &lt;h2&gt;SystemTap Dyninst runtime use cases&lt;/h2&gt; &lt;p&gt;The following examples use the Dyninst runtime, such as when the kernel runtime is not available, or to make use of the Dyninst runtime in its own right. The examples can be cut, pasted, and executed using the given stap command lines.&lt;/p&gt; &lt;p&gt;Many programs such as Python, Perl, TCL, editors, and web servers employ event loops. The next example demonstrates parameter changes in the Python event loop. This Python program, pyexample.py, converts Celsius to Fahrenheit. This example requires the installation of debuginfo for the &lt;code&gt;python3-libs&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;stap --dyninst varwatch.stp 'process("/usr/lib64/libpython3.8.so.1.0").statement("PyEval_EvalCodeEx@*:*")' '$$parms' -c '/usr/bin/python3 pyexample.py 35'&lt;/pre&gt; &lt;p&gt;where &lt;code&gt;varwatch.stp&lt;/code&gt; is:&lt;/p&gt; &lt;pre&gt;global var% probe $1 { if (@defined($2)) { newvar = $2; if (var[tid()] != newvar) { printf("%s[%d] %s %s:\n", execname(), tid(), pp(), @2); println(newvar); var[tid()] = newvar; } } }&lt;/pre&gt; &lt;p&gt;What is &lt;code&gt;PyEval_EvalCodeEx@*:*&lt;/code&gt; and how did we determine it? Developers place static probes in the Python executable. For more details, go to the following User space static probes section. One of these probes is &lt;code&gt;function__entry&lt;/code&gt;. For this probe, search the sources for that marker, and extrapolate from that point. Once arriving at the &lt;code&gt;PyEval_EvalCodeEx&lt;/code&gt; function, the &lt;code&gt;@*:#&lt;/code&gt; portion indicates where to set a probe for each statement in the function. Then, using this information, we can set a probe that accumulates time statistics for the Python event loop:&lt;/p&gt; &lt;pre&gt;stap --dyninst /work/scox/stap"PyEval_EvalCodeEx")' -c /scripts/func_time_stats.stp 'process("/usr/lib64/libpython3.8.so.1.0").function('/usr/bin/python3 pyexample.py 35'&lt;/pre&gt; &lt;p&gt;where &lt;code&gt;func_time_stats.stp&lt;/code&gt; is:&lt;/p&gt; &lt;pre&gt;global start, intervals probe $1 { start[tid()] = gettimeofday_us() } probe $1.return { t = gettimeofday_us() old_t = start[tid()] if (old_t) intervals &amp;#60;&amp;#60;&amp;#60; t - old_t delete start[tid()] }&lt;/pre&gt; &lt;p&gt;where the output is:&lt;/p&gt; &lt;pre&gt;35 Celsius is 95.0 Farenheit intervals min:0us avg:49us max:6598us count:146 variance:297936 value |-------------------------------------------------- count 0 |@@@@@ 10 1 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 64 2 |@@@@@@@@@@@@@@@@@@@@@ 43&lt;/pre&gt; &lt;p&gt;Then set a probe at the static marker &lt;code&gt;cmd__entry&lt;/code&gt; in &lt;code&gt;libtcl8.6.so&lt;/code&gt; to display the arguments in the TCL event loop:&lt;/p&gt; &lt;pre&gt;stap --dyninst -e 'probe process("/usr/lib64/libtcl8.6.so").mark("cmd__entry") {printf("%s %#lxd %#lxd\n",$$name,$arg1,$arg2)}' -c /usr/bin/tclsh8.6&lt;/pre&gt; &lt;h2&gt;Systemtap Dyninst runtime overview&lt;/h2&gt; &lt;p&gt;The Dyninst runtime differs in that it creates a C user space source and generates a shared object. The Dyninst runtime does not require special privilege.&lt;/p&gt; &lt;p&gt;The SystemTap+Dyninst operation diagram in Figure 1 compares both approaches.&lt;/p&gt; &lt;div id="attachment_828187" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/11/josh-dyninst-11.jpg"&gt;&lt;img aria-describedby="caption-attachment-828187" class="wp-image-828187" src="https://developers.redhat.com/blog/wp-content/uploads/2020/11/josh-dyninst-11-1024x768.jpg" alt="SystemTap and Dyninst operation flow " width="640" height="480" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/11/josh-dyninst-11-1024x768.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/11/josh-dyninst-11-300x225.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/11/josh-dyninst-11-768x576.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/11/josh-dyninst-11.jpg 1102w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-828187" class="wp-caption-text"&gt;Figure 1. Comparison of SystemTap and Dyninst runtime environment and operation.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The SystemTap Dyninst runtime environment supports a subset of the probes that the kernel runtime supports. The following sections provide an overview of some of the probe families and the status of those probes in the Dyninst runtime.&lt;/p&gt; &lt;h3&gt;User space probes&lt;/h3&gt; &lt;p&gt;User space probes are source-level probes that require debuginfo.  &lt;code&gt;varwatch.stp&lt;/code&gt; and &lt;code&gt;func_time_stats.stp&lt;/code&gt; exemplify these types of probes. The kernel runtime in the examples can be invoked without the &lt;code&gt;-c&lt;/code&gt; COMMAND option. This allows the example to be associated with any process on the system running &lt;code&gt;/usr/bin/ex&lt;/code&gt;. The Dyninst runtime cannot be used for this type of system monitoring. It requires association with a specific process that is specified either with the &lt;code&gt;-x&lt;/code&gt; PID or &lt;code&gt;-c&lt;/code&gt; COMMAND option.&lt;/p&gt; &lt;h3&gt;User space variable access&lt;/h3&gt; &lt;p&gt;SystemTap can access many types of variables. However, the Dyninst runtime cannot access certain types of variables that the default kernel runtime can access. Typically, these are global variables, which require tracking the virtual memory address, a feature not present in the Dyninst runtime. For example, accessing the global variable &lt;code&gt;Rows&lt;/code&gt; in the &lt;code&gt;ex&lt;/code&gt; program:&lt;/p&gt; &lt;pre&gt;stap --dyninst -e 'probe process("/usr/bin/ex").function("do_cmdline") {printf("%d\n",@var("Rows@main.c"))}'&lt;/pre&gt; &lt;p&gt;gives the error:&lt;/p&gt; &lt;pre&gt;&lt;em&gt;semantic error: VMA-tracking is only supported by the kernel runtime (PR15052): operator '@var' at &amp;#60;input&amp;#62;:1:68 source: probe process("/usr/bin/ex").function("do_cmdline") {printf("%d\n",@var("Rows@main.c"))}&lt;/em&gt;&lt;/pre&gt; &lt;p&gt;When this error occurs, avoid attempting to access that particular variable. The -L option mentioned in the next section allows you to find and display a possible alternate context variable.&lt;/p&gt; &lt;h3&gt;User space static probes&lt;/h3&gt; &lt;p&gt;SystemTap can probe symbolic static instrumentation that is compiled into programs and shared libraries. The previous probe &lt;code&gt;mark ("cmd__entry")&lt;/code&gt; is an example of this type of probe. Developers place static probes at useful locations. SystemTap can list available static probes in an executable or shared object. For example, to list static probes in the &lt;code&gt;libtcl&lt;/code&gt; shared object:&lt;/p&gt; &lt;pre&gt;stap -L 'process("/usr/lib64/libtcl8.6.so").mark("*")'&lt;/pre&gt; &lt;pre&gt;&lt;em&gt;process("/usr/lib64/libtcl8.6.so").mark("cmd__entry") $arg1:long $arg2:long $arg3:long&lt;/em&gt; process("/usr/lib64/libtcl8.6.so").mark("cmd__return") $arg1:long $arg2:long ...&lt;/pre&gt; &lt;p&gt;A &lt;code&gt;$argN&lt;/code&gt; reference in a static probe might receive a VMA tracking error. When this is the case, avoid that particular &lt;code&gt;$argN&lt;/code&gt; reference.&lt;/p&gt; &lt;h3&gt;Timer probes&lt;/h3&gt; &lt;p&gt;There is a timer family of probes. Jiffies timers, called &lt;code&gt;timer.jiffie&lt;/code&gt;, is a kernel feature not available in the Dyninst runtime. There is also another type of timer available in the Dyninst runtime called unit-of-time timers or &lt;code&gt;timer.ms(N)&lt;/code&gt;. For example, to exit SystemTap after two seconds, a SystemTap script might include:&lt;/p&gt; &lt;pre&gt;probe timer.ms(2000) {exit()}&lt;/pre&gt; &lt;h3&gt;Kernel space probes&lt;/h3&gt; &lt;p&gt;When the example SystemTap command line is:&lt;/p&gt; &lt;pre&gt;stap -e 'probe kernel.function("bio*") { printf ("%s -&amp;#62; %s\n", thread_indent(1), probefunc())}'&lt;/pre&gt; &lt;p&gt;Any process that invokes a kernel function with the wildcard name &lt;code&gt;bio*&lt;/code&gt; displays the name of the probe. If the &lt;code&gt;–runtime=Dyninst&lt;/code&gt; option is given, then it can&amp;#8217;t succeed because the Dyninst runtime cannot probe kernel functions. This is also true of the &lt;code&gt;syscall.*&lt;/code&gt; and &lt;code&gt;perf.*&lt;/code&gt; family of probes that require kernel functionality.&lt;/p&gt; &lt;h3&gt;Tapsets&lt;/h3&gt; &lt;p&gt;A tapset is a script that is designed for reuse and installed into a special directory. The Dyninst runtime doesn&amp;#8217;t implement all of the tapsets. For example, if a SystemTap script attempts to use the &lt;code&gt;task_utime&lt;/code&gt; tapset, SystemTap warns that the tapset containing &lt;code&gt;task_utime&lt;/code&gt; is not available in the Dyninst runtime:&lt;/p&gt; &lt;pre&gt;stap --dyninst -e 'probe process("/usr/bin/ex").function("do_cmdline") {printf("%d\n",task_utime())}'&lt;/pre&gt; &lt;pre&gt;&lt;em&gt;semantic error: unresolved function task_utime (similar: ctime, qs_time, tz_ctime, tid, uid): identifier 'task_utime' at &amp;#60;input&amp;#62;:1:68&lt;/em&gt;&lt;/pre&gt; &lt;h2&gt;Probe summary&lt;/h2&gt; &lt;p&gt;The Dyninst runtime does not support the following probe types:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;kernel.*&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;perf.*&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;tapset.*&lt;/code&gt; (Dyninst runtime implements some, but not all scripts)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Dyninst runtime supports the following probe types:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;process.*&lt;/code&gt;  (if specified with -x or -c)&lt;/li&gt; &lt;li&gt;&lt;code&gt;process.* {...@var("VAR")}&lt;/code&gt;  (if no VMA issues)&lt;/li&gt; &lt;li&gt;&lt;code&gt;process.mark&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;timer.ms&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Micro benchmark&lt;/h3&gt; &lt;p&gt;This example compares the runtime of a microbenchmark that has eight threads, with each running a null loop 10,000,000 times, and with a probe firing in each loop. The timing is measured in microseconds, and it begins after SystemTap completes the probe setup, then the benchmark begins executing. The system time for the Dyninst runtime, as compared to the system time for the kernel runtime, reflects the fact that the Dyninst runtime runs the probes in user space.&lt;/p&gt; &lt;pre&gt; Dyninst Runtime Kernel Module Runtime User 7,864,521 8,712,623 System 4,808,738 12,049,084&lt;/pre&gt; &lt;h3&gt;Dyninst execution details&lt;/h3&gt; &lt;p&gt;Dyninst can instrument a running, dynamic process. It can also instrument a process that hasn&amp;#8217;t run yet, called a static process.&lt;br /&gt; Dyninst inserts the instrumentation code using ptrace, although the instrumentation runs in the process. Because of ptrace limitations, Dyninst can probe only one instance of a process. In addition, the program inserting the instrumentation is called the mutator, and the program being instrumented is called the mutatee.&lt;/p&gt; &lt;p&gt;A typical mutator can perform the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Attach to a running process, create a new process, or load a nonrunning executable.&lt;/li&gt; &lt;li&gt;Build the Dyninst image of the process.&lt;/li&gt; &lt;li&gt;Find the function to be instrumented: &lt;ul&gt; &lt;li&gt;Creates a Dyninst snippet, which is an abstraction that describes the instrumented code.&lt;/li&gt; &lt;li&gt;For example, in a function call: &lt;ul&gt; &lt;li&gt;Create a Dyninst call snippet.&lt;/li&gt; &lt;li&gt;Create Dyninst argument snippets.&lt;/li&gt; &lt;li&gt;Translate the snippets into instructions and insert them at the probe point.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Dynamic instrumentation occurs when the process is attached or created, and then it continues.&lt;/li&gt; &lt;li&gt;Static instrumentation occurs when the process doesn&amp;#8217;t execute. In this case, it creates a new executable.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A Dyninst snippet creates an abstraction of the code that is inserted at a probe point. The snippet can create or access variables or types, access registers, and logical, conditional, and arithmetic expressions. Dyninst converts the snippets into instructions and inserts them at the probe point.&lt;/p&gt; &lt;p&gt;In the case of SystemTap, the mutator is the SystemTap tool stapdyn. This tool creates snippets that call SystemTap handlers, which are defined in a shared object and correspond to the probe points. The Dyninst snippets do not handle the SystemTap probe. Instead, the probe handlers perform that function and the Dyninst snippets call those handlers.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;The SystemTap Dyninst backend enables the use of SystemTap to probe a user application, without requiring any special privileges. Probes run with minimal overhead because they run in user space. The Dyninst runtime is an alternative the next time a user space application needs probing. &lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fusing-the-systemtap-dyninst-runtime-environment%2F&amp;#38;linkname=Using%20the%20SystemTap%20Dyninst%20runtime%20environment" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fusing-the-systemtap-dyninst-runtime-environment%2F&amp;#38;linkname=Using%20the%20SystemTap%20Dyninst%20runtime%20environment" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fusing-the-systemtap-dyninst-runtime-environment%2F&amp;#38;linkname=Using%20the%20SystemTap%20Dyninst%20runtime%20environment" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fusing-the-systemtap-dyninst-runtime-environment%2F&amp;#38;linkname=Using%20the%20SystemTap%20Dyninst%20runtime%20environment" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fusing-the-systemtap-dyninst-runtime-environment%2F&amp;#38;linkname=Using%20the%20SystemTap%20Dyninst%20runtime%20environment" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fusing-the-systemtap-dyninst-runtime-environment%2F&amp;#38;linkname=Using%20the%20SystemTap%20Dyninst%20runtime%20environment" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fusing-the-systemtap-dyninst-runtime-environment%2F&amp;#38;linkname=Using%20the%20SystemTap%20Dyninst%20runtime%20environment" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fusing-the-systemtap-dyninst-runtime-environment%2F&amp;#038;title=Using%20the%20SystemTap%20Dyninst%20runtime%20environment" data-a2a-url="https://developers.redhat.com/blog/2021/04/16/using-the-systemtap-dyninst-runtime-environment/" data-a2a-title="Using the SystemTap Dyninst runtime environment"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/16/using-the-systemtap-dyninst-runtime-environment/"&gt;Using the SystemTap Dyninst runtime environment&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/gVbvMRdmxd8" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;SystemTap (stap) uses a command-line interface (CLI) and a scripting language to write instrumentation for a live running kernel or a user space application. A SystemTap script associates handlers with named events. This means, when a specified event occurs, the default SystemTap kernel runtime runs the handler in the kernel as if it is a [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/16/using-the-systemtap-dyninst-runtime-environment/"&gt;Using the SystemTap Dyninst runtime environment&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/16/using-the-systemtap-dyninst-runtime-environment/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">828147</post-id><dc:creator>Stan Cox</dc:creator><dc:date>2021-04-16T07:00:23Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/16/using-the-systemtap-dyninst-runtime-environment/</feedburner:origLink></entry><entry><title>Deploying the Mosquitto MQTT message broker on Red Hat OpenShift, Part 1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/b9-1qBxLWtQ/" /><category term="Containers" /><category term="Event-Driven" /><category term="Internet of Things" /><category term="Kubernetes" /><category term="message broker" /><category term="Mosquitto" /><category term="MQTT" /><category term="openshift" /><author><name>kboone</name></author><id>https://developers.redhat.com/blog/?p=820577</id><updated>2021-04-16T07:00:08Z</updated><published>2021-04-16T07:00:08Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://mosquitto.org/"&gt;Mosquitto&lt;/a&gt; is a lightweight message broker that supports the &lt;a target="_blank" rel="nofollow" href="https://mqtt.org/"&gt;Message Queuing Telemetry Transport &lt;/a&gt; (MQTT) protocol. Mosquitto is widely used in &lt;a target="_blank" rel="nofollow" href="/blog/category/iot/"&gt;Internet of Things&lt;/a&gt; (IoT) and telemetry applications, where a fully-featured message broker like &lt;a target="_blank" rel="nofollow" href="/products/amq/overview"&gt;Red Hat AMQ&lt;/a&gt; would be unnecessarily burdensome. Mosquitto also finds a role as a message bus for interprocess communication in distributed systems. Because it avoids complex features, Mosquitto is easy to tune and handles substantial application workloads with relatively modest memory and CPU resources.&lt;/p&gt; &lt;p&gt;There are essentially two stages to making Mosquitto available on &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. First, you need to &lt;a target="_blank" rel="nofollow" href="/topics/containers"&gt;containerize&lt;/a&gt; the application in a way that is broadly compatible with OpenShift. Part of containerization involves installing the container image in a repository, from which OpenShift can download it. Second, you need to deploy the containerized image in a pod, providing whatever properties and configuration are necessary for the specific installation. The first half of this article shows how to build Mosquitto into an image suitable for use in a container. The second half will show you how to configure and deploy the Mosquitto image on OpenShift.&lt;/p&gt; &lt;h2&gt;Mosquitto basics&lt;/h2&gt; &lt;p&gt;Mosquitto&amp;#8217;s messaging model is publish-subscribe, like a &amp;#8220;topic&amp;#8221; in Java Message Service (JMS) terminology. In fact, the Mosquitto documentation uses the term &amp;#8220;topic&amp;#8221; for its message destinations. In the terminology of AMQ 7 (and ActiveMQ Artemis, on which AMQ 7 is based), Mosquitto destinations are &lt;em&gt;non-durable anycast addresses&lt;/em&gt; by default. Mosquitto communicates only over MQTT: a lightweight, payload-neutral, non-transactional wire protocol.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: A detailed description of Mosquitto is beyond the scope of this article. For more information, please see the &lt;a target="_blank" rel="nofollow" href="https://mosquitto.org/"&gt;Mosquitto website&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It&amp;#8217;s relatively easy to deploy Mosquitto in an OpenShift pod: Mosquitto has (and needs) no clustering support, so it lends itself to simple deployment methods. While you might find uses for Mosquitto on OpenShift, this article focuses as much on software packaging in an OpenShift-compatible way as it does on Mosquitto itself. Many widely-used software packages were not designed for cloud or container operation; as a result, they present challenges to creating a maintainable, configurable installation.&lt;/p&gt; &lt;h2&gt;Using Mosquitto with OpenShift&lt;/h2&gt; &lt;p&gt;For this article, I make certain assumptions about how Mosquitto (or any other lightweight application) will be used on OpenShift:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You want to minimize pod startup time and container image size. There&amp;#8217;s no benefit to running multiple replicas of Mosquitto, so in the event of a crash, you need a new pod to spin up in milliseconds.&lt;/li&gt; &lt;li&gt;The application will be used both from within the OpenShift cluster, using a service, and from outside it, using a route. Compatibility with the OpenShift routing framework necessitates the use of Transport Layer Security (TLS) encryption in the wire protocol, as I will discuss later.&lt;/li&gt; &lt;li&gt;Client access will be authenticated rather than be left completely open, both for intercluster and external clients. In this example, I&amp;#8217;ll use simple user-password authentication, although Mosquitto also supports client certificates.&lt;/li&gt; &lt;li&gt;Although the installer will need to customize the deployment on OpenShift, an installation that uses only the defaults in the container image should be functional enough for testing. It should not be mandatory to provide a complex configuration just to get started. That means, for example, providing the container image with default TLS certificates.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the instructions, I&amp;#8217;m also assuming you have a local workstation installation of Mosquitto&amp;#8211;or at least, the Mosquitto test clients&amp;#8211;for testing. On Fedora and &lt;a target="_blank" rel="nofollow" href="/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) systems, typing &lt;code&gt;dnf install mosquitto&lt;/code&gt; in the shell should do the trick.&lt;/p&gt; &lt;p&gt;For the sake of brevity, I won&amp;#8217;t display all the relevant files in their entirety. You can obtain all of these files from my &lt;a target="_blank" rel="nofollow" href="https://github.com/kevinboone/mosquitto-openshift"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Note that I&amp;#8217;ll use &lt;a target="_blank" rel="nofollow" href="https://podman.io/"&gt;Podman&lt;/a&gt; to build and run images in my demonstration. Docker and Buildah should also work without modifying the example.&lt;/p&gt; &lt;h2&gt;Configuration files for containerization&lt;/h2&gt; &lt;p&gt;Mosquitto needs at least four files, or sets of files, to provide a rudimentary service on OpenShift. You might need to configure these files at installation time:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Credentials for at least one user&lt;/li&gt; &lt;li&gt;A set of TLS certificates&lt;/li&gt; &lt;li&gt;A configuration file for the broker&lt;/li&gt; &lt;li&gt;A startup script&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A standard Mosquitto installation provides default locations for configuration files. In this example, however, I&amp;#8217;ve chosen to put all the files that might need to be modified by the deployer into a single directory in the container image. I&amp;#8217;ve chosen the home directory for the user account that will own the broker. I&amp;#8217;m using the directory &lt;code&gt;/myuser&lt;/code&gt;. The directory&amp;#8217;s name is arbitrary, but administration is simpler if all the files that an installer will need to change are in the same directory.&lt;/p&gt; &lt;p&gt;In the source repository, all the relevant configuration files and certificates are in the &lt;code&gt;files&lt;/code&gt; directory, and will be copied to &lt;code&gt;/myuser&lt;/code&gt; in the image when it is built.&lt;/p&gt; &lt;p&gt;We&amp;#8217;ll also need a Dockerfile to build the container image from a base image, the Mosquitto binaries, and the configuration files. It will be possible (perhaps &lt;em&gt;necessary&lt;/em&gt; is a better word) to test the generated image on the local workstation before installing it on OpenShift.&lt;/p&gt; &lt;h3&gt;Credentials file&lt;/h3&gt; &lt;p&gt;Mosquitto provides a simple user-password authentication mechanism based on a credentials file. The credentials file is in a proprietary format, containing hashed passwords. Mosquitto provides a utility named &lt;code&gt;mosquitto_passwd&lt;/code&gt; for editing the file.&lt;/p&gt; &lt;p&gt;The default image provides one user  named &lt;code&gt;admin&lt;/code&gt; with the password &lt;code&gt;admin&lt;/code&gt;, in a credentials file called &lt;code&gt;passwd&lt;/code&gt;. I created this file as follows:&lt;/p&gt; &lt;pre&gt;$ touch files/passwd $ mosquitto_passwd -b files/passwd admin admin &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: In Part 2, I will explain how to provide different credentials when deploying the image on OpenShift.&lt;/p&gt; &lt;p&gt;To create multiple users with different privileges, edit the main configuration file and add users to the credentials file.&lt;/p&gt; &lt;h3&gt;TLS certificates&lt;/h3&gt; &lt;p&gt;Mosquitto requires at least three certificates for a practical installation. Like all the other files that an installer may override, these will be installed into the image in the &lt;code&gt;/myuser&lt;/code&gt; directory. These certificates are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A root certification authority (CA) certificate against which all the others will be authenticated. In my example, this file is named &lt;code&gt;ca.crt&lt;/code&gt;. &lt;em&gt;You must share this certificate with clients&lt;/em&gt;. I&amp;#8217;m generating this file using OpenSSL in the example, but in a production installation, it might be a trusted certificate from a commercial CA.&lt;/li&gt; &lt;li&gt;A server certificate authenticated by the CA. This will be called &lt;code&gt;server.crt&lt;/code&gt;. This is the certificate that Mosquitto will supply to clients during the TLS handshake.&lt;/li&gt; &lt;li&gt;The primary key certificate corresponding to the server certificate. This will be called &lt;code&gt;server.key&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All these certificates must be in PEM format and are easily generated using the &lt;code&gt;openssl&lt;/code&gt; utility.&lt;/p&gt; &lt;p&gt;The following example shows the commands that I used to generate the certificates in the default image. You might want to use similar commands to bake different certificates into the image; however, I envisage all these files being overridden in OpenShift at installation time, from a secret or configmap (more about this in Part 2).&lt;/p&gt; &lt;pre&gt;$ openssl req -new -x509 -days 3650 -extensions v3_ca -keyout files/ca.key \ -out files/ca.crt -subj "/O=acme/CN=com" $ openssl genrsa -out files/server.key 2048 $ openssl req -new -out files/server.csr -key files/server.key -subj "/O=acme2/CN=com" $ openssl x509 -req -in files/server.csr -CA files/ca.crt -CAkey files/ca.key \ -CAcreateserial -out files/server.crt -days 3650 $ openssl rsa -in files/server.key -out files/server.key $ rm files/ca.key files/ca.srl files/server.csr $ chmod 644 files/server.key &lt;/pre&gt; &lt;h3&gt;Main configuration file&lt;/h3&gt; &lt;p&gt;The main configuration file specifies the locations of the credentials files and certificate files and creates TCP listeners on ports 1883 (plaintext) and 8883 (TLS). These port numbers are conventional for MQTT, and the Mosquitto utilities assume they are in use unless otherwise specified. Here is the &lt;code&gt;mosquitto.conf&lt;/code&gt;configuration file for this example.&lt;/p&gt; &lt;pre&gt;# Port for plaintext communication port 1883 # Location of the credentials file password_file /myuser/passwd # Port and certificates for TLS encrypted communication listener 8883 certfile /myuser/server.crt cafile /myuser/ca.crt keyfile /myuser/server.key&lt;/pre&gt; &lt;p&gt;Note that all the referenced files are in &lt;code&gt;/myuser&lt;/code&gt;. This directory will be copied and populated during the image build.&lt;/p&gt; &lt;p&gt;In practice, this configuration file will probably need to be overridden. If it is, the new version will probably also need to specify a credentials file and certificate files.&lt;/p&gt; &lt;h3&gt;Startup script&lt;/h3&gt; &lt;p&gt;Starting up is trivially simple in this example. The script &lt;code&gt;startup.sh&lt;/code&gt; just runs the &lt;code&gt;mosquitto&lt;/code&gt; command, specifying the main configuration file:&lt;/p&gt; &lt;pre&gt;#!/bin/sh mosquitto -c /myuser/mosquitto.conf &lt;/pre&gt; &lt;h2&gt;Building, testing, and publishing the image&lt;/h2&gt; &lt;p&gt;To create the smallest possible image, I&amp;#8217;m using Alpine Linux as a base. Alpine is a minimal Linux specifically designed for containers. To further reduce its size and complexity, Alpine uses the MUSL C library rather than the Glibc that is almost ubiquitous in Linux. This decision mostly limits Alpine&amp;#8217;s use to applications that are available in the Alpine repository or can be built from source.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: At the time of writing, the latest Alpine release is 3.12. The corresponding Alpine repository doesn&amp;#8217;t have a binary package for Mosquitto, so to simplify the image build, I&amp;#8217;m specifying Alpine 3.11. To use a later version of Alpine, if the package is not present in the repository, you&amp;#8217;ll need to build Mosquitto from source. Building Mosquitto is not difficult, but to build a version for Alpine, you must build against the MUSL C library. A straightforward way to do this is to install a desktop version of Alpine Linux, perhaps in a virtual machine, and do the build there. Another approach is to build Mosquitto in-image, using a multistage build process to eliminate all the build tools from the final image. There&amp;#8217;s a lot more information on this multistage build process in my article &lt;a target="_blank" rel="nofollow" href="/blog/2020/08/27/developing-micro-microservices-in-c-on-red-hat-openshift/"&gt;&lt;em&gt;Developing micro-microservices in C on Red Hat OpenShift&lt;/em&gt;&lt;/a&gt;. Here, I&amp;#8217;m happy to use the binary package from the Alpine repository.&lt;/p&gt; &lt;p&gt;Here is a simple Dockerfile that creates the image, using Alpine Linux as a base:&lt;/p&gt; &lt;pre&gt;FROM alpine:3.11 RUN addgroup -g 1000 mygroup &amp;#38;&amp;#38; \ adduser -G mygroup -u 1000 -h /myuser -D myuser &amp;#38;&amp;#38; \ chown -R myuser:mygroup /myuser &amp;#38;&amp;#38; \ apk --no-cache add mosquitto WORKDIR /myuser COPY files/* /myuser/ USER myuser EXPOSE 1883 8883 CMD ["/myuser/start.sh"] &lt;/pre&gt; &lt;p&gt;There&amp;#8217;s nothing remarkable about this Dockerfile. It creates a single user, exposes the ports that Mosquitto uses, and specifies our startup script as the entry point.&lt;/p&gt; &lt;p&gt;The total size of the final image is only about 7 MB.&lt;/p&gt; &lt;h3&gt;Building and testing the image locally&lt;/h3&gt; &lt;p&gt;Build the image using:&lt;/p&gt; &lt;pre&gt;$ podman build . &lt;/pre&gt; &lt;p&gt;Get the ID of the new image (using, for example, &lt;code&gt;podman image list&lt;/code&gt;) and then run it, specifying the port mappings. In the following command, replace the italicized word &lt;em&gt;image&lt;/em&gt; with the ID of your image:&lt;/p&gt; &lt;pre&gt;$ podman run -it -port 1883:1883 -p 8883:8883 &lt;em&gt;image&lt;/em&gt;&lt;/pre&gt; &lt;p&gt;This command exposes the plaintext and TLS ports. We can test the image using the Mosquitto test clients &lt;code&gt;mosquitto_pub&lt;/code&gt; (publish) and/or &lt;code&gt;mosquitto_sub&lt;/code&gt; (subscribe). For example, to publish a message using the plaintext listener, enter:&lt;/p&gt; &lt;pre&gt;$ mosquitto_pub -t foo -m "Some text" -u admin -P admin &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;-t foo&lt;/code&gt; option specifies the Mosquitto topic, while &lt;code&gt;-m&lt;/code&gt; specifies the data to send. The user (&lt;code&gt;-u&lt;/code&gt; option) and password (&lt;code&gt;-P&lt;/code&gt; option) must match the values given in the &lt;code&gt;mosquitto_passwd&lt;/code&gt; command when creating the credentials file earlier. Note that the hostname and port default to &lt;code&gt;localhost&lt;/code&gt; and 1883, which are appropriate in this case.&lt;/p&gt; &lt;p&gt;To test the TLS listener, you&amp;#8217;ll need to specify the CA certificate. The certificate is in the source repository&amp;#8217;s &lt;code&gt;files&lt;/code&gt; directory and in the image (I&amp;#8217;ll discuss how to obtain the certificate from the OpenShift pod in Part 2):&lt;/p&gt; &lt;pre&gt;$ mosquitto_sub -t foo --cafile files/ca.crt --insecure -u admin -P admin &lt;/pre&gt; &lt;p&gt;Again, the defaults for host and port will be appropriate. You need the &lt;code&gt;--insecure&lt;/code&gt; switch to override certificate hostname checks; this is because the server certificate in the image has the hostname &lt;code&gt;acme.com&lt;/code&gt;, not &lt;code&gt;localhost&lt;/code&gt;.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Mosquitto is non-durable by default. So if you want to test that messages can be produced and consumed. you&amp;#8217;ll need to start the consumer before the producer.&lt;/p&gt; &lt;h3&gt;Publishing the image&lt;/h3&gt; &lt;p&gt;If the image works adequately on a workstation, you must publish it to a repository from which OpenShift can download it. The procedures for doing this are outside the scope of this article. I&amp;#8217;ve published my image to &lt;code&gt;quay.io&lt;/code&gt; using the following &lt;code&gt;podman&lt;/code&gt; procedure:&lt;/p&gt; &lt;pre&gt;$ podman tag &amp;#60;image-id&amp;#62; mosquitto-ephemeral:0.1a $ podman login quay.io... $ podman push mosquitto-ephemeral:0.1a quay.io/kboone/mosquitto-ephemeral &lt;/pre&gt; &lt;p&gt;This is a public repository, so feel free to use my image for testing if you don&amp;#8217;t want to build your own. I&amp;#8217;m using the tag &lt;code&gt;ephemeral&lt;/code&gt; in the image name to indicate that this implementation does not support durable messaging.&lt;/p&gt; &lt;p&gt;Of course, you could publish the image to another public image repository, or a private, institutional repository, or directly to the OpenShift internal repository if you&amp;#8217;re willing to expose it as a route. For detailed steps, see the article &lt;a target="_blank" rel="nofollow" href="http://kevinboone.me/podman_deploy.html"&gt;&lt;em&gt;Using Podman to deploy an image directly to OpenShift 4&lt;/em&gt;&lt;/a&gt; on my website.&lt;/p&gt; &lt;p&gt;However you publish the image, you&amp;#8217;ll need to know the repository URI for the OpenShift deployment. In this example using the &lt;code&gt;quay.io&lt;/code&gt; repository, that URI is:&lt;/p&gt; &lt;pre&gt;quay.io/kboone/mosquitto-ephemeral:latest &lt;/pre&gt; &lt;h2&gt;Conclusion to Part 1&lt;/h2&gt; &lt;p&gt;This article described some of the design considerations that are relevant to deploying a lightweight application on OpenShift. I don&amp;#8217;t claim that my approach is the only one you could use, or even particularly optimal. It is, however, straightforward enough for a demonstration, and might highlight a variety of issues that packagers and deployers should consider. Look for Part 2, where we will deploy the Mosquitto image we built in this article on OpenShift.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fdeploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1%2F&amp;#38;linkname=Deploying%20the%20Mosquitto%20MQTT%20message%20broker%20on%20Red%20Hat%20OpenShift%2C%20Part%201" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fdeploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1%2F&amp;#38;linkname=Deploying%20the%20Mosquitto%20MQTT%20message%20broker%20on%20Red%20Hat%20OpenShift%2C%20Part%201" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fdeploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1%2F&amp;#38;linkname=Deploying%20the%20Mosquitto%20MQTT%20message%20broker%20on%20Red%20Hat%20OpenShift%2C%20Part%201" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fdeploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1%2F&amp;#38;linkname=Deploying%20the%20Mosquitto%20MQTT%20message%20broker%20on%20Red%20Hat%20OpenShift%2C%20Part%201" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fdeploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1%2F&amp;#38;linkname=Deploying%20the%20Mosquitto%20MQTT%20message%20broker%20on%20Red%20Hat%20OpenShift%2C%20Part%201" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fdeploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1%2F&amp;#38;linkname=Deploying%20the%20Mosquitto%20MQTT%20message%20broker%20on%20Red%20Hat%20OpenShift%2C%20Part%201" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fdeploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1%2F&amp;#38;linkname=Deploying%20the%20Mosquitto%20MQTT%20message%20broker%20on%20Red%20Hat%20OpenShift%2C%20Part%201" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F16%2Fdeploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1%2F&amp;#038;title=Deploying%20the%20Mosquitto%20MQTT%20message%20broker%20on%20Red%20Hat%20OpenShift%2C%20Part%201" data-a2a-url="https://developers.redhat.com/blog/2021/04/16/deploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1/" data-a2a-title="Deploying the Mosquitto MQTT message broker on Red Hat OpenShift, Part 1"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/16/deploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1/"&gt;Deploying the Mosquitto MQTT message broker on Red Hat OpenShift, Part 1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/b9-1qBxLWtQ" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Mosquitto is a lightweight message broker that supports the Message Queuing Telemetry Transport  (MQTT) protocol. Mosquitto is widely used in Internet of Things (IoT) and telemetry applications, where a fully-featured message broker like Red Hat AMQ would be unnecessarily burdensome. Mosquitto also finds a role as a message bus for interprocess communication in distributed systems. [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/16/deploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1/"&gt;Deploying the Mosquitto MQTT message broker on Red Hat OpenShift, Part 1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/16/deploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">820577</post-id><dc:creator>kboone</dc:creator><dc:date>2021-04-16T07:00:08Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/16/deploying-the-mosquitto-mqtt-message-broker-on-red-hat-openshift-part-1/</feedburner:origLink></entry><entry><title type="html">Eclipse Vert.x 3.9.7 released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/UZNp6wlyaUg/eclipse-vert-x-3-9-7" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-3-9-7</id><updated>2021-04-16T00:00:00Z</updated><content type="html">Eclipse Vert.x version 3.9.7 has just been released. It fixes quite a few bugs that have been reported by the community.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/UZNp6wlyaUg" height="1" width="1" alt=""/&gt;</content><dc:creator>Julien Viet</dc:creator><feedburner:origLink>https://vertx.io/blog/eclipse-vert-x-3-9-7</feedburner:origLink></entry><entry><title>Containerize .NET for Red Hat OpenShift: Linux containers and .NET Core</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/noqZgBcpcwc/" /><category term=".NET" /><category term="Containers" /><category term="Kubernetes" /><category term="Linux" /><category term=".NET Core" /><category term="Dockerfile" /><category term="Linux containers" /><category term="S2I" /><author><name>Don Schenck</name></author><id>https://developers.redhat.com/blog/?p=866247</id><updated>2021-04-15T07:00:59Z</updated><published>2021-04-15T07:00:59Z</published><content type="html">&lt;p&gt;When &lt;a target="_blank" rel="nofollow" href="/topics/dotnet"&gt;.NET&lt;/a&gt; was released to the open source world (November 12, 2014—not that I remember the date or anything), it didn&amp;#8217;t just bring .NET to open source; it brought open source to .NET. &lt;a target="_blank" rel="nofollow" href="/topics/containers"&gt;Linux containers&lt;/a&gt; were one of the then-burgeoning, now-thriving technologies that became available to .NET developers. At that time, it was &amp;#8220;docker, docker, docker&amp;#8221; all the time. Now, it&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="/blog/2019/02/21/podman-and-buildah-for-docker-users/"&gt;Podman and Buildah&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="/topics/serverless-architecture"&gt;serverless&lt;/a&gt;, and &amp;#8230; well, you get the idea. Things have progressed, and your .NET applications can progress, as well.&lt;/p&gt; &lt;p&gt;This article is part of a series introducing &lt;a target="_blank" rel="nofollow" href="/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/"&gt;three ways to containerize .NET applications on Red Hat OpenShift&lt;/a&gt;. I&amp;#8217;ll start with a high-level overview of Linux containers and .NET Core, then discuss a couple of ways to build and containerize .NET Core applications and deploy them on OpenShift.&lt;/p&gt; &lt;p&gt;&lt;span id="more-866247"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;How Linux containers work&lt;/h2&gt; &lt;p&gt;To start, let&amp;#8217;s take a high-level look at how Linux containers work.&lt;/p&gt; &lt;p&gt;A &lt;em&gt;container&lt;/em&gt; is where you run your image. That is, you build an image, and when it runs, it runs in the container. The container runs on the host system and has access to the host&amp;#8217;s kernel, regardless of the host&amp;#8217;s Linux distribution (distro). For example, You might be running your container on a &lt;a target="_blank" rel="nofollow" href="/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) server, but the image running in the container was built to include Debian as the runtime Linux distro. To go one step further, and make things more complicated, let&amp;#8217;s say the image was constructed (for instance, using &lt;code&gt;podman build...&lt;/code&gt;) on a Fedora machine.&lt;/p&gt; &lt;p&gt;For a high-level visualization, think of a container as a virtual machine (VM) running an application. (It&amp;#8217;s not, but the analogy might help.) Now, consider the diagram in Figure 1.&lt;/p&gt; &lt;div id="attachment_867517" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/containers.png"&gt;&lt;img aria-describedby="caption-attachment-867517" class="wp-image-867517" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/containers.png" alt="A diagram showing the difference between a virtual machine and a container." width="640" height="248" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/containers.png 988w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/containers-300x116.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/containers-768x298.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-867517" class="wp-caption-text"&gt;Figure 1: Virtual machines versus containers.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;What .NET Core allows you to do is build a .NET application that runs on a Linux distro. Your options include the &lt;a target="_blank" rel="nofollow" href="/blog/2021/04/13/how-to-pick-the-right-container-base-image/"&gt;RHEL with the Universal Base Images&lt;/a&gt;, or UBI, distro.&lt;/p&gt; &lt;p&gt;Now, consider the code at this GitHub repository (repo): &lt;a target="_blank" rel="nofollow" href="https://github.com/donschenck/qotd-csharp"&gt;https://github.com/donschenck/qotd-csharp&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In the file, &amp;#8220;Dockerfile,&amp;#8221; we can see the following reference as our base image:&lt;/p&gt; &lt;p&gt;&lt;code&gt;FROM registry.access.redhat.com/ubi8/dotnet-31:3.1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;We&amp;#8217;re starting with the Red Hat UBI image, with .NET Core 3.1 already installed, but we could run the resulting image on a container hosted by a different Linux distro.&lt;/p&gt; &lt;h2&gt;Build here, run elsewhere&lt;/h2&gt; &lt;p&gt;The bottom line is, you can build .NET Core applications where you choose—Windows, Linux, macOS—and deploy the resulting image to OpenShift. This is my preferred way of developing .NET Core &lt;a target="_blank" rel="nofollow" href="/topics/microservices"&gt;microservices&lt;/a&gt;. I like the idea of building and testing an image on my local machine, then distributing the image—unchanged in any way—to OpenShift. This approach gives me confidence in the fidelity of the bits being executed. It also eliminates the &amp;#8220;but it worked on my machine&amp;#8221; scenario.&lt;/p&gt; &lt;p&gt;The &amp;#8220;Build here, run elsewhere&amp;#8221; approach is basically three steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Build (and test) the image on your local machine.&lt;/li&gt; &lt;li&gt;Push the image to an image registry.&lt;/li&gt; &lt;li&gt;Deploy the image to your OpenShift cluster using the &lt;b&gt;Container Image&lt;/b&gt; option.&lt;/li&gt; &lt;/ol&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Remember&lt;/strong&gt;: Because you are using .NET Core, you are building and running using Linux. You can build from your PC (macOS, Linux, or Windows), and the image will run on OpenShift.&lt;/p&gt; &lt;p&gt;After building the image on my local machine and pushing it to my own image registry, Figure 2 shows what it looks like to create the application in my cluster. &lt;/p&gt; &lt;div id="attachment_885087" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/deploy-image.png"&gt;&lt;img aria-describedby="caption-attachment-885087" class="wp-image-885087" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/deploy-image.png" alt="Options for deploying an application using an image from an image registry." width="640" height="663" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/deploy-image.png 763w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/deploy-image-290x300.png 290w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-885087" class="wp-caption-text"&gt;Figure 2: Options for deploying the image in OpenShift.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Build there, run there&lt;/h2&gt; &lt;p&gt;Another option is to build your image using Red Hat OpenShift. Actually, OpenShift provides two ways to build your image: Source-to-Image (S2I) or building from a Dockerfile. A third option is building from a container image, which involves building the image &lt;em&gt;outside of&lt;/em&gt; Red Hat OpenShift. We&amp;#8217;ll discuss that option in a future article. For now, take a look at the overview in Figure 3.&lt;/p&gt; &lt;div id="attachment_868117" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/two-build-from-source-options.png"&gt;&lt;img aria-describedby="caption-attachment-868117" class="wp-image-868117" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/two-build-from-source-options.png" alt="Two ways to build from source: S2I or building from a Dockerfile." width="640" height="177" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/two-build-from-source-options.png 821w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/two-build-from-source-options-300x83.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/two-build-from-source-options-768x212.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-868117" class="wp-caption-text"&gt;Figure 3: Two ways to build from source.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Building an image from Git&lt;/h3&gt; &lt;p&gt;This option uses OpenShift&amp;#8217;s S2I technology to fetch your source code, build it, and deploy it as an image (running in a container) to OpenShift. This option does not require you to create the build configuration (meaning, a Dockerfile); you simply create the code and let OpenShift take care of the rest. You are obviously denied the flexibility of using a build configuration, but it is a fast and easy way to build an image. This approach might work well for a good portion of your needs. And if it works, why mess with it?&lt;/p&gt; &lt;h3&gt;Building from a Dockerfile&lt;/h3&gt; &lt;p&gt;This &amp;#8220;same but different&amp;#8221; option relies on the &lt;code&gt;docker build&lt;/code&gt; command inside your OpenShift cluster and uses the Dockerfile included with your source code as the build instructions. Basically, it&amp;#8217;s like running the build on your local machine. Figure 4 shows a screen capture of a project that includes the Dockerfile.&lt;/p&gt; &lt;div id="attachment_884967" style="width: 288px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/qotd-file-list.png"&gt;&lt;img aria-describedby="caption-attachment-884967" class="wp-image-884967 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/qotd-file-list.png" alt="A list of files in the project." width="278" height="530" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/qotd-file-list.png 278w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/qotd-file-list-157x300.png 157w" sizes="(max-width: 278px) 100vw, 278px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-884967" class="wp-caption-text"&gt;Figure 4: A project including a Dockerfile.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Here are the Dockerfile&amp;#8217;s contents:&lt;/p&gt; &lt;pre&gt;FROM registry.access.redhat.com/ubi8/dotnet-31:3.1 USER 1001 RUN mkdir qotd-csharp WORKDIR qotd-csharp ADD . . RUN dotnet publish -c Release EXPOSE 10000 CMD ["dotnet", "./bin/Release/netcoreapp3.0/publish/qotd-csharp.dll"] &lt;/pre&gt; &lt;p&gt;Because we have a valid Dockerfile, we can use the &lt;b&gt;Import from Dockerfile&lt;/b&gt; option. Note that we need to make sure we have the correct port—in this case, 10000, as shown in Figure 5.&lt;/p&gt; &lt;div id="attachment_884947" style="width: 622px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/import-from-dockerfile.png"&gt;&lt;img aria-describedby="caption-attachment-884947" class="wp-image-884947 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/import-from-dockerfile.png" alt="The parameters needed to build an image from a Dockerfile." width="612" height="792" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/import-from-dockerfile.png 612w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/import-from-dockerfile-232x300.png 232w" sizes="(max-width: 612px) 100vw, 612px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-884947" class="wp-caption-text"&gt;Figure 5: Options for importing from a Dockerfile.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Importing from the Dockerfile results in the image being built and deployed, and it creates a route to expose the application.&lt;/p&gt; &lt;h2&gt;Conclusion: It&amp;#8217;s just like any other language&lt;/h2&gt; &lt;p&gt;The truth is, using .NET Core with Linux is just like most any other development language. You build, test, push, and deploy. It&amp;#8217;s that simple. No special magic is needed. You are a first-class citizen in the world of Linux, containers, microservices, Kubernetes, Red Hat OpenShift, and so on. Welcome in.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: Curious about moving existing .NET Framework code to .NET 5 (.NET Core)? Check out &lt;a target="_blank" rel="nofollow" href="https://devblogs.microsoft.com/dotnet/introducing-the-net-upgrade-assistant-preview/?ocid=AID3017126"&gt;Microsoft&amp;#8217;s .NET Upgrade Assistant&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fcontainerize-net-for-red-hat-openshift-linux-containers-and-net-core%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Linux%20containers%20and%20.NET%20Core" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fcontainerize-net-for-red-hat-openshift-linux-containers-and-net-core%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Linux%20containers%20and%20.NET%20Core" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fcontainerize-net-for-red-hat-openshift-linux-containers-and-net-core%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Linux%20containers%20and%20.NET%20Core" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fcontainerize-net-for-red-hat-openshift-linux-containers-and-net-core%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Linux%20containers%20and%20.NET%20Core" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fcontainerize-net-for-red-hat-openshift-linux-containers-and-net-core%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Linux%20containers%20and%20.NET%20Core" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fcontainerize-net-for-red-hat-openshift-linux-containers-and-net-core%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Linux%20containers%20and%20.NET%20Core" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fcontainerize-net-for-red-hat-openshift-linux-containers-and-net-core%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Linux%20containers%20and%20.NET%20Core" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fcontainerize-net-for-red-hat-openshift-linux-containers-and-net-core%2F&amp;#038;title=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Linux%20containers%20and%20.NET%20Core" data-a2a-url="https://developers.redhat.com/blog/2021/04/15/containerize-net-for-red-hat-openshift-linux-containers-and-net-core/" data-a2a-title="Containerize .NET for Red Hat OpenShift: Linux containers and .NET Core"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/15/containerize-net-for-red-hat-openshift-linux-containers-and-net-core/"&gt;Containerize .NET for Red Hat OpenShift: Linux containers and .NET Core&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/noqZgBcpcwc" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;When .NET was released to the open source world (November 12, 2014—not that I remember the date or anything), it didn&amp;#8217;t just bring .NET to open source; it brought open source to .NET. Linux containers were one of the then-burgeoning, now-thriving technologies that became available to .NET developers. At that time, it was &amp;#8220;docker, docker, [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/15/containerize-net-for-red-hat-openshift-linux-containers-and-net-core/"&gt;Containerize .NET for Red Hat OpenShift: Linux containers and .NET Core&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/15/containerize-net-for-red-hat-openshift-linux-containers-and-net-core/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">866247</post-id><dc:creator>Don Schenck</dc:creator><dc:date>2021-04-15T07:00:59Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/15/containerize-net-for-red-hat-openshift-linux-containers-and-net-core/</feedburner:origLink></entry><entry><title>Vulnerability analysis for Golang applications with Red Hat CodeReady Dependency Analytics</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ocBSxBVMXE8/" /><category term="CI/CD" /><category term="Go" /><category term="Java" /><category term="Security" /><category term="VS Code" /><category term="CodeReady Dependency Analytics" /><category term="Golang" /><category term="Snyk" /><author><name>Parag Dave</name></author><id>https://developers.redhat.com/blog/?p=879447</id><updated>2021-04-15T07:00:58Z</updated><published>2021-04-15T07:00:58Z</published><content type="html">&lt;p&gt;Red Hat CodeReady Dependency Analytics, powered by &lt;a target="_blank" rel="nofollow" href="https://snyk.io/product/vulnerability-database/"&gt;Snyk Intel Vulnerability database&lt;/a&gt;, helps developers find, identify, and fix security vulnerabilities in their code. In the latest 0.3.2 release, we focused on supporting vulnerability analysis for &lt;a href="https://developers.redhat.com/blog/category/go/"&gt;Golang&lt;/a&gt; application dependencies, providing easier access to vulnerability details uniquely known to Snyk, and other user experience improvements.&lt;/p&gt; &lt;h2&gt;Vulnerability analysis for Golang applications&lt;/h2&gt; &lt;p&gt;With this release, developers can run vulnerability analyses for their Golang application stacks. The analysis identifies vulnerabilities at the Golang module and package levels, including direct and transitive dependencies. It also supports semver and pseudo versions.&lt;/p&gt; &lt;h3&gt;go.mod file component analysis&lt;/h3&gt; &lt;p&gt;Opening a &lt;code&gt;go.mod&lt;/code&gt; file in the IDE editor window triggers the plug-in&amp;#8217;s component analysis. The component analysis highlights the vulnerabilities for each dependency listed in the &lt;code&gt;go.mod&lt;/code&gt; file. The plug-in provides the option to examine each vulnerability&amp;#8217;s details. You can then remedy the vulnerability by switching to the recommended version using the &lt;strong&gt;Quick Fix&lt;/strong&gt; option.&lt;/p&gt; &lt;p&gt;Editing, saving, or reopening the &lt;code&gt;go.mod&lt;/code&gt; file will also initiate the component analysis.&lt;/p&gt; &lt;p&gt;Figure 1 shows the output from the IDE editor&amp;#8217;s component analysis, with a summary of the findings in the notification window in the bottom-right corner. This summary lists the total number of vulnerabilities and exploits found in the manifest.&lt;/p&gt; &lt;div id="attachment_879467" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-1.png"&gt;&lt;img aria-describedby="caption-attachment-879467" class="wp-image-879467 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-1-1024x536.png" alt="Output from a go.mod file component analysis, with a notification in the bottom left alerting the user to vulnerabilities." width="640" height="335" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-1-1024x536.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-1-300x157.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-1-768x402.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-1.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-879467" class="wp-caption-text"&gt;Figure 1: Output from a &lt;code&gt;go.mod&lt;/code&gt; file component analysis.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The vulnerabilities and exploits are highlighted in the IDE editor with red and blue underlines. Hovering over an issue displays a diagnostic summary of the specific package&amp;#8217;s vulnerability profile, as Figure 2 illustrates.&lt;/p&gt; &lt;div id="attachment_894057" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/Golang_Blog_Image.png"&gt;&lt;img aria-describedby="caption-attachment-894057" class="wp-image-894057" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/Golang_Blog_Image-271x300.png" alt="Sample output in the IDE editor with various diagnostic messages displayed." width="640" height="709" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/Golang_Blog_Image-271x300.png 271w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/Golang_Blog_Image-768x851.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/Golang_Blog_Image.png 899w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-894057" class="wp-caption-text"&gt;Figure 2: Various diagnostic messages displayed for a specific package.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Each diagnostic message provides the following information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The total number of vulnerabilities&lt;/li&gt; &lt;li&gt;The number of Snyk-specific security advisories&lt;/li&gt; &lt;li&gt;A count of known exploits&lt;/li&gt; &lt;li&gt;The highest severity level across all vulnerabilities&lt;/li&gt; &lt;li&gt;The number of packages and modules used in the source&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Details about security vulnerabilities&lt;/h2&gt; &lt;p&gt;To access a detailed report of security issues in a &lt;code&gt;go.mod&lt;/code&gt; file (see Figure 3), click the button in the notification window; then right-click the &lt;code&gt;go.mod&lt;/code&gt; file and select &lt;strong&gt;Dependency Analytics Report&lt;/strong&gt;. Alternatively, you can click the pie-chart icon labeled &lt;strong&gt;Open Vulnerability Report&lt;/strong&gt; in the top-right corner.&lt;/p&gt; &lt;div id="attachment_879487" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-3.png"&gt;&lt;img aria-describedby="caption-attachment-879487" class="wp-image-879487 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-3-1024x540.png" alt="The detailed vulnerability report displays the total number of vulnerabilities and vulnerable dependencies in the go.mod file." width="640" height="338" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-3-1024x540.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-3-300x158.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-3-768x405.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-3.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-879487" class="wp-caption-text"&gt;Figure 3: Detailed vulnerability report for a &lt;code&gt;go.mod&lt;/code&gt; file.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;div id="attachment_879497" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-4.png"&gt;&lt;img aria-describedby="caption-attachment-879497" class="wp-image-879497 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-4-1024x536.png" alt="The detailed vulnerability report providing more information about each vulnerable dependency." width="640" height="335" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-4-1024x536.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-4-300x157.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-4-768x402.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-4.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-879497" class="wp-caption-text"&gt;Figure 4: Details about a vulnerable dependency.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Accessing vulnerability details&lt;/h2&gt; &lt;p&gt;The CodeReady Dependency Analytics plug-in is powered by Snyk Intel, a vulnerability database provided by Snyk. By signing up for a free Snyk account, you can view each vulnerability&amp;#8217;s known exploits and details about vulnerabilities uniquely known to Snyk.&lt;/p&gt; &lt;p&gt;Figure 5 shows how to register with Snyk and save your token with the CodeReady Dependency Analytics plug-in.&lt;/p&gt; &lt;div id="attachment_879547" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-5-1.png"&gt;&lt;img aria-describedby="caption-attachment-879547" class="wp-image-879547 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-5-1-1024x576.png" alt="A window prompting the user to enter an existing Snyk token or register a new one." width="640" height="360" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-5-1-1024x576.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-5-1-300x169.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-5-1-768x432.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Figure-5-1.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-879547" class="wp-caption-text"&gt;Figure 5: Enter an existing Snyk token or register a new one to view each vulnerability’s known exploits.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 6 shows the vulnerabilities and exploit details after the Snyk token has been saved.&lt;/p&gt; &lt;div id="attachment_894097" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/Screen-Shot-2021-04-09-at-9.45.07-AM.png"&gt;&lt;img aria-describedby="caption-attachment-894097" class="wp-image-894097 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/Screen-Shot-2021-04-09-at-9.45.07-AM-1024x679.png" alt="Viewing details of vulnerabilities that are uniquely identified by Snyk after the token is saved." width="640" height="424" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/Screen-Shot-2021-04-09-at-9.45.07-AM-1024x679.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/Screen-Shot-2021-04-09-at-9.45.07-AM-300x199.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/Screen-Shot-2021-04-09-at-9.45.07-AM-768x509.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-894097" class="wp-caption-text"&gt;Figure 6: Viewing details of vulnerabilities that are uniquely identified by Snyk after the token is saved.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Enabling telemetry&lt;/h2&gt; &lt;p&gt;The Dependency Analytics extension is now integrated with the Red Hat Commons extension. This enables Red Hat to collect telemetry specific to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The type of operating system running Visual Studio Code&lt;/li&gt; &lt;li&gt;The development language of the manifest file scanned by Dependency Analytics&lt;/li&gt; &lt;li&gt;The time and frequency of vulnerability report generation using stack analysis and component-level analysis&lt;/li&gt; &lt;li&gt;Registration with Snyk&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;By collecting telemetry, Red Hat can gain valuable feedback about the extension&amp;#8217;s usage patterns and provide future enhancements for developers.&lt;/p&gt; &lt;p&gt;The Dependency Analytics extension only collects telemetry if developers opt in to enable it. You can opt in any time by clicking &lt;strong&gt;Accept&lt;/strong&gt; (see Figure 7) or selecting the checkbox for Red Hat Telemetry under &lt;strong&gt;Preferences&lt;/strong&gt; → &lt;strong&gt;Settings&lt;/strong&gt; (see Figure 8).&lt;/p&gt; &lt;div id="attachment_879607" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-879607" class="wp-image-879607 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/optin-request-1024x237.png" alt="A pop-up window asks the user to opt in to allow Red Hat to collect usage data." width="640" height="148" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/optin-request-1024x237.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/optin-request-300x69.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/optin-request-768x178.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/optin-request.png 1280w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-879607" class="wp-caption-text"&gt;Figure 7: Opt in to sending telemetry data or decline via the pop-up window.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;div id="attachment_879587" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-879587" class="wp-image-879587 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-08-at-3.50.32-PM-1024x284.png" alt="In the extension's settings, the checkbox is clicked to enable usage data to be sent to Red Hat." width="640" height="178" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-08-at-3.50.32-PM-1024x284.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-08-at-3.50.32-PM-300x83.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screen-Shot-2021-03-08-at-3.50.32-PM-768x213.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-879587" class="wp-caption-text"&gt;Figure 8: Configuring telemetry in the extension&amp;#8217;s settings.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Get started with Red Hat CodeReady Dependency Analytics&lt;/h2&gt; &lt;p&gt;Red Hat CodeReady Dependency Analytics is available as a plug-in for &lt;a target="_blank" rel="nofollow" href="https://marketplace.visualstudio.com/items?itemName=redhat.fabric8-analytics"&gt;Visual Studio Code&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://www.eclipse.org/che/"&gt;Eclipse Che&lt;/a&gt;, &lt;a href="https://developers.redhat.com/products/codeready-workspaces/overview"&gt;Red Hat CodeReady Workspaces&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="https://plug-ins.jetbrains.com/plug-in/12541-dependency-analytics/"&gt;JetBrains IntelliJ-based IDEs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To get started with the CodeReady Dependency Analytics IDE extension or provide feedback, check out the following links.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://marketplace.visualstudio.com/items?itemName=redhat.fabric8-analytics"&gt;Get the Visual Studio Code extension for CodeReady Dependency Analytics&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://plug-ins.jetbrains.com/plug-in/12541-dependency-analytics/"&gt;Get the IntelliJ-based IDE extension for CodeReady Dependency Analytics&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Access CodeReady Dependency Analytics via the &lt;a target="_blank" rel="nofollow" href="https://github.com/fabric8-analytics/fabric8-analytics-server/tree/master/api_specs/v2"&gt;fabric8-analytics-server&lt;/a&gt; API.&lt;/li&gt; &lt;li&gt;Provide your feedback via the &lt;a target="_blank" rel="nofollow" href="https://github.com/fabric8-analytics/fabric8-analytics-vscode-extension/issues"&gt;Git issues repository.&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;About Snyk&lt;/h2&gt; &lt;p&gt;Snyk is a developer-first security company that helps software-driven businesses develop fast and stay secure. Snyk finds and fixes vulnerabilities and license violations in open source dependencies and container images. Snyk&amp;#8217;s solution is built on Snyk Intel, a comprehensive, proprietary vulnerability database maintained by an expert security research team in Israel and London. With tight integration into existing developer workflows, source control, and CI/CD pipelines, Snyk enables efficient security workflows and reduces mean-time-to-fix. For more information or to get started with Snyk for free today, visit &lt;a target="_blank" rel="nofollow" href="https://snyk.io"&gt;https://snyk.io&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fvulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics%2F&amp;#38;linkname=Vulnerability%20analysis%20for%20Golang%20applications%20with%20Red%20Hat%20CodeReady%20Dependency%20Analytics" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fvulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics%2F&amp;#38;linkname=Vulnerability%20analysis%20for%20Golang%20applications%20with%20Red%20Hat%20CodeReady%20Dependency%20Analytics" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fvulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics%2F&amp;#38;linkname=Vulnerability%20analysis%20for%20Golang%20applications%20with%20Red%20Hat%20CodeReady%20Dependency%20Analytics" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fvulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics%2F&amp;#38;linkname=Vulnerability%20analysis%20for%20Golang%20applications%20with%20Red%20Hat%20CodeReady%20Dependency%20Analytics" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fvulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics%2F&amp;#38;linkname=Vulnerability%20analysis%20for%20Golang%20applications%20with%20Red%20Hat%20CodeReady%20Dependency%20Analytics" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fvulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics%2F&amp;#38;linkname=Vulnerability%20analysis%20for%20Golang%20applications%20with%20Red%20Hat%20CodeReady%20Dependency%20Analytics" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fvulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics%2F&amp;#38;linkname=Vulnerability%20analysis%20for%20Golang%20applications%20with%20Red%20Hat%20CodeReady%20Dependency%20Analytics" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Fvulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics%2F&amp;#038;title=Vulnerability%20analysis%20for%20Golang%20applications%20with%20Red%20Hat%20CodeReady%20Dependency%20Analytics" data-a2a-url="https://developers.redhat.com/blog/2021/04/15/vulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics/" data-a2a-title="Vulnerability analysis for Golang applications with Red Hat CodeReady Dependency Analytics"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/15/vulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics/"&gt;Vulnerability analysis for Golang applications with Red Hat CodeReady Dependency Analytics&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ocBSxBVMXE8" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Red Hat CodeReady Dependency Analytics, powered by Snyk Intel Vulnerability database, helps developers find, identify, and fix security vulnerabilities in their code. In the latest 0.3.2 release, we focused on supporting vulnerability analysis for Golang application dependencies, providing easier access to vulnerability details uniquely known to Snyk, and other user experience improvements. Vulnerability analysis for [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/15/vulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics/"&gt;Vulnerability analysis for Golang applications with Red Hat CodeReady Dependency Analytics&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/15/vulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">879447</post-id><dc:creator>Parag Dave</dc:creator><dc:date>2021-04-15T07:00:58Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/15/vulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics/</feedburner:origLink></entry><entry><title>Fail fast with Opossum circuit breaker in Node.js</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/woqL5RR7nsQ/" /><category term="JavaScript" /><category term="Kubernetes" /><category term="Microservices" /><category term="Node.js" /><category term="@redhat/opossum" /><category term="circuit breaker pattern" /><category term="Opossum Circuit Breaker" /><author><name>Lucas Holmquist</name></author><id>https://developers.redhat.com/blog/?p=880677</id><updated>2021-04-15T07:00:11Z</updated><published>2021-04-15T07:00:11Z</published><content type="html">&lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="/topics/microservices"&gt;microservices&lt;/a&gt; pattern is pretty standard for today&amp;#8217;s software architecture. Microservices let you break up your application into small chunks and avoid having one giant monolith. The only problem is that if one of these services fails, it could have a cascading effect on your whole architecture.&lt;/p&gt; &lt;p&gt;Luckily, there is another pattern that can help with this issue: The circuit breaker pattern.&lt;/p&gt; &lt;p&gt;This article explains what a circuit breaker is and how to use the pattern in your &lt;a target="_blank" rel="nofollow" href="/topics/nodejs"&gt;Node.js&lt;/a&gt; applications. We&amp;#8217;ll use &lt;a target="_blank" rel="nofollow" href="https://www.npmjs.com/package/opossum"&gt;Opossum&lt;/a&gt;, a Node.js implementation of the circuit breaker pattern.&lt;/p&gt; &lt;h2&gt;What is a circuit breaker?&lt;/h2&gt; &lt;p&gt;Before we dive into an example, let&amp;#8217;s quickly define a circuit breaker and how to use the pattern in your code.&lt;/p&gt; &lt;p&gt;You might already be familiar with circuit breakers if you’ve tried to run too many household appliances at once. The lights go out due to an overwhelming influx of electricity. To restore power, you need to go down to the basement, find the electrical box, search for the breaker that “tripped,” and reset it. Circuit breakers protect your residence by shutting down during an electrical surge.&lt;/p&gt; &lt;p&gt;The circuit breaker pattern works similarly when dealing with microservices that communicate over a network. Its purpose is to reduce the impact of a service that is running too slowly or that can’t be reached due to a network failure. The circuit breaker monitors for such failures. Once failures reach a particular threshold, the circuit “trips,” and any call made after that either returns an error or adopts a fallback response. Then, after a set time has passed, the circuit breaker makes test calls to the affected services. If the calls are successful, the circuit closes, and traffic starts flowing again.&lt;/p&gt; &lt;p&gt;Circuit breakers are especially important when multiple services depend on each other. If one service fails, it could take down the whole architecture. Remember the first Death Star explosion in the &lt;em&gt;Star Wars&lt;/em&gt; movie series? A good circuit breaker might have avoided that.&lt;/p&gt; &lt;h2&gt;What is Opossum?&lt;/h2&gt; &lt;p&gt;Opossum is a circuit breaker for Node.js. When things start to fail, opossum plays dead and fails fast. If you want to, you can provide a fallback function to be executed when in the failure state.&lt;/p&gt; &lt;p&gt;Opossum has been a community project since late 2016, and it now has more than 70,000 downloads per week. It is supported by the &lt;a target="_blank" rel="nofollow" href="https://nodeshift.dev"&gt;Nodeshift&lt;/a&gt; community. Recently, Red Hat has released a fully supported version of Opossum that is distributed through Red Hat’s customer registry as &lt;code&gt;@redhat/opossum&lt;/code&gt;. Opossum will always be a community project, but if you would like to know that the version you are using has Red Hat&amp;#8217;s support, then the &lt;code&gt;@redhat/opossum&lt;/code&gt; version might be for you. You can learn more about Red Hat&amp;#8217;s Node.js offerings &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/articles/5641561" &gt;here&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;The following sections show how to add this module to an application and how to use it to protect your microservices.&lt;/p&gt; &lt;h2&gt;Adding Red Hat Opossum to your application&lt;/h2&gt; &lt;p&gt;Adding the &lt;code&gt;@redhat/opossum&lt;/code&gt; module to your application is just like adding any other Node.js module, with one small change. Because you will download this module from the Red Hat customer registry, you need to tell &lt;code&gt;npm&lt;/code&gt; to download any modules with the &lt;code&gt;@redhat&lt;/code&gt; namespace from the Red Hat registry while continuing to download all other modules from the upstream NPM registry.&lt;/p&gt; &lt;p&gt;To start, add a &lt;code&gt;.npmrc&lt;/code&gt; file in your application&amp;#8217;s root directory. The file should look something like this:&lt;/p&gt; &lt;pre&gt;@redhat:registry=https://npm.registry.redhat.com registry=https://registry.npmjs.org &lt;/pre&gt; &lt;p&gt;With this file in place, you can run the following command successfully:&lt;/p&gt; &lt;pre&gt;$ npm install @redhat/opossum &lt;/pre&gt; &lt;p&gt;To &lt;code&gt;require&lt;/code&gt; the module in your application, insert the same kind of statement you would for every other Node.js module:&lt;/p&gt; &lt;pre&gt;const CircuitBreaker = require(‘@redhat/opossum’) &lt;/pre&gt; &lt;p&gt;Now, let&amp;#8217;s take a look at an example.&lt;/p&gt; &lt;h2&gt;Example: Opossum circuit breaker for Node.js&lt;/h2&gt; &lt;p&gt;For this example, we are going to use the Nodeshift &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift-starters/nodejs-circuit-breaker-redhat"&gt;Circuit Breaker Starter Application&lt;/a&gt;.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: This example works the same way on both the community and Red Hat versions of Opossum.&lt;/p&gt; &lt;p&gt;The example consists of two simple Node.js microservices, so let&amp;#8217;s look at them both.&lt;/p&gt; &lt;h3&gt;The greeting service&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;greeting-service&lt;/code&gt; is the application&amp;#8217;s entry point. A simple web page makes a call to the &lt;code&gt;greeting&lt;/code&gt; REST endpoint. This endpoint then makes a call, wrapped in a circuit breaker, to the second service. The web page also has a button to let you toggle the name service (which I will introduce shortly) on or off to simulate a network failure.&lt;/p&gt; &lt;p&gt;Here&amp;#8217;s the code responsible for the greeting service:&lt;/p&gt; &lt;pre&gt;... // We require Opossum const Opossum = require('@redhat/opossum'); … // Set some circuit breaker options const circuitOptions = { timeout: 3000, // If name service takes longer than .3 seconds, trigger a failure errorThresholdPercentage: 50, // When 50% of requests fail, trip the circuit resetTimeout: 10000 // After 10 seconds, try again. }; … // Use a circuit breaker for the name service and define fallback function const circuit = new Opossum(nameService, circuitOptions); circuit.fallback(_ =&amp;#62; 'Fallback'); … // Greeting API app.get('/api/greeting', (request, response) =&amp;#62; { // Using the Circuits fire method to execute the call to the name service circuit.fire(`${nameServiceHost}/api/name`).then(name =&amp;#62; { response.send({ content: `Hello, ${name}`, time: new Date() }); }).catch(console.error); }); &lt;/pre&gt; &lt;p&gt;Next, we pass the &lt;code&gt;nameService&lt;/code&gt; function as the first argument to the circuit breaker. It looks something like the following, which is a standard call to another endpoint using &lt;code&gt;axios&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;'use strict'; const axios = require('axios'); module.exports = endpoint =&amp;#62; { return new Promise((resolve, reject) =&amp;#62; { axios.get(endpoint) .then(response =&amp;#62; { if (response.status !== 200) { return reject(new Error(`Expected status code 200, instead got ${response.status}`)); } resolve(response.data); }) .catch(reject); }); }; &lt;/pre&gt; &lt;h3&gt;The name service&lt;/h3&gt; &lt;p&gt;The other microservice, &lt;code&gt;name-service&lt;/code&gt;, is a REST endpoint that sends back a response based on the on or off toggle I mentioned before.&lt;/p&gt; &lt;p&gt;Starting the application is straightforward. From the repository&amp;#8217;s root directory, run the &lt;code&gt;./start-localhost.sh&lt;/code&gt; file to bring up the two Node.js processes. The script will also attempt to open a web browser to the location of the running application.&lt;/p&gt; &lt;p&gt;Pressing the &lt;b&gt;invoke&lt;/b&gt; button on the web page contacts the first endpoint. The endpoint sends back a response saying whether it could contact the second service or had to use the fallback. You can click the toggle button to simulate a network failure.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has shown how a circuit breaker helps to reduce unexpected failures in microservices. You can use the &lt;code&gt;@redhat/opossum&lt;/code&gt; module to add this pattern to your Node.js applications. To find out more about this new supported offering, check out the article &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/articles/5890311"&gt;&lt;em&gt;Opossum: Fully supported circuit breaker module for Red Hat build of Node.js&lt;/em&gt;&lt;/a&gt; on the Red Hat Customer Portal.&lt;/p&gt; &lt;p&gt;See these resources to learn more about the topics discussed in this article:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For more more about circuit breaking, see Microservice Architecture&amp;#8217;s &lt;a href="https://microservices.io/patterns/reliability/circuit-breaker.html"&gt;introduction to the circuit breaker pattern&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Also see Martin Fowler&amp;#8217;s very good &lt;a target="_blank" rel="nofollow" href="https://martinfowler.com/bliki/CircuitBreaker.html"&gt;article about the circuit breaker pattern.&lt;/a&gt;&lt;/li&gt; &lt;li&gt;See the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/webassets/avalon/d/red_hat_build_of_node/opossum/5.0.0/jsdoc/"&gt;Opossum API documentation&lt;/a&gt; for more about what you can do with &lt;code&gt;@redhat/opossum&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Visit the &lt;a target="_blank" rel="nofollow" href="/topics/nodejs"&gt;Node.js landing page&lt;/a&gt; to find out what else Red Hat is doing with Node.js.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Ffail-fast-with-opossum-circuit-breaker-in-node-js%2F&amp;#38;linkname=Fail%20fast%20with%20Opossum%20circuit%20breaker%20in%20Node.js" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Ffail-fast-with-opossum-circuit-breaker-in-node-js%2F&amp;#38;linkname=Fail%20fast%20with%20Opossum%20circuit%20breaker%20in%20Node.js" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Ffail-fast-with-opossum-circuit-breaker-in-node-js%2F&amp;#38;linkname=Fail%20fast%20with%20Opossum%20circuit%20breaker%20in%20Node.js" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Ffail-fast-with-opossum-circuit-breaker-in-node-js%2F&amp;#38;linkname=Fail%20fast%20with%20Opossum%20circuit%20breaker%20in%20Node.js" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Ffail-fast-with-opossum-circuit-breaker-in-node-js%2F&amp;#38;linkname=Fail%20fast%20with%20Opossum%20circuit%20breaker%20in%20Node.js" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Ffail-fast-with-opossum-circuit-breaker-in-node-js%2F&amp;#38;linkname=Fail%20fast%20with%20Opossum%20circuit%20breaker%20in%20Node.js" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Ffail-fast-with-opossum-circuit-breaker-in-node-js%2F&amp;#38;linkname=Fail%20fast%20with%20Opossum%20circuit%20breaker%20in%20Node.js" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F15%2Ffail-fast-with-opossum-circuit-breaker-in-node-js%2F&amp;#038;title=Fail%20fast%20with%20Opossum%20circuit%20breaker%20in%20Node.js" data-a2a-url="https://developers.redhat.com/blog/2021/04/15/fail-fast-with-opossum-circuit-breaker-in-node-js/" data-a2a-title="Fail fast with Opossum circuit breaker in Node.js"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/15/fail-fast-with-opossum-circuit-breaker-in-node-js/"&gt;Fail fast with Opossum circuit breaker in Node.js&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/woqL5RR7nsQ" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;The microservices pattern is pretty standard for today&amp;#8217;s software architecture. Microservices let you break up your application into small chunks and avoid having one giant monolith. The only problem is that if one of these services fails, it could have a cascading effect on your whole architecture. Luckily, there is another pattern that can help [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/15/fail-fast-with-opossum-circuit-breaker-in-node-js/"&gt;Fail fast with Opossum circuit breaker in Node.js&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/15/fail-fast-with-opossum-circuit-breaker-in-node-js/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">880677</post-id><dc:creator>Lucas Holmquist</dc:creator><dc:date>2021-04-15T07:00:11Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/15/fail-fast-with-opossum-circuit-breaker-in-node-js/</feedburner:origLink></entry></feed>
