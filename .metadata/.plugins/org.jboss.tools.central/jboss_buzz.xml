<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Migrating existing applications to Quarkus with Migration Toolkit for Applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/wXc6iYd_5DQ/" /><author><name /></author><id>https://quarkus.io/blog/migrating-existing-apps-to-quarkus-with-mta/</id><updated>2021-03-29T00:00:00Z</updated><content type="html">The evolution of Java Java is a language that never ceases to impress me. From its conception, to the first Java Virtual Machines with the premise of “write once run anywhere”, to Tomcat or to the Enterprise Edition standards (whether J2EE, or Java EE) it’s an ever evolving language. With...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/wXc6iYd_5DQ" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/migrating-existing-apps-to-quarkus-with-mta/</feedburner:origLink></entry><entry><title type="html">Scorecard Editor</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/THbKogNgaY8/scorecard-editor.html" /><author><name>Michael Anstis</name></author><id>https://blog.kie.org/2021/03/scorecard-editor.html</id><updated>2021-03-25T09:37:22Z</updated><content type="html">We are delighted to announce that preliminary work on a PMML (4.4) Scorecard Editor has completed. Overview A VSCode extension has been published to the and can be added to your VSCode installation. The release is considered alpha and primarily aimed at providing a channel to gather feedback. The journey to provide a capable editor has started but the road is long and winding. Please install, kick the tyres and provide feedback as this is what will drive change. EDITING MADE SIMPLE Just click, edit and move on. All changes can be undone/redone. Model settings IN-LINE VALIDATION Made a mistake? Errors are shown in-line and can be undone and corrected as needed. Inline errors INTEGRATION WITH VSCODE Errors also integrate with the Problems panel. Problems panel DEFINING DATA FIELDS Use the "Set Data Dictionary" dialog to define Data Fields. Click on a row to edit a Data Field’s extended properties Basic properties Click on "Edit properties" for more advanced extended properties. Editing extended properties DEFINING MINING FIELDS AND OUTPUT FIELDS Similarly the Mining Schema and Outputs can be defined. Mining Field basic properties Mining Field extended properties Output Field basic properties Output Field extended properties CHARACTERISTICS AND ATTRIBUTES The way in which you interact with the editor remains consistent. Both Characteristics and Attributes are authored similarly to the different types of fields. PREDICATES The PMML Specification allows for the definition of complex compound expressions. We therefore decided to provide a context-aware, auto-complete predicate editor. This is where we really would like to receive feedback. YOUR EDITOR NEEDS YOU! Real-life predicates seem to be much more simple and we find ourselves at a cross-road. Do we invest in completing the text-based predicate editor or do we investigate a different approach? Your feedback counts! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/THbKogNgaY8" height="1" width="1" alt=""/&gt;</content><dc:creator>Michael Anstis</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/scorecard-editor.html</feedburner:origLink></entry><entry><title>Integrate Red Hat’s single sign-on technology 7.4 with Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/xq9iR3Qr2fw/" /><category term="Containers" /><category term="DevOps" /><category term="Kubernetes" /><category term="Linux" /><category term="openshift" /><category term="OpenShift 4.5" /><category term="persistent storage" /><category term="postgresql" /><category term="single sign-on" /><author><name>orivat</name></author><id>https://developers.redhat.com/blog/?p=791907</id><updated>2021-03-25T07:00:59Z</updated><published>2021-03-25T07:00:59Z</published><content type="html">&lt;p&gt;In this article, you will learn how to integrate &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat’s single sign-on technology&lt;/a&gt; 7.4 with &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift 4&lt;/a&gt;. For this integration, we&amp;#8217;ll use the &lt;a target="_blank" rel="nofollow" href="https://www.postgresql.org/"&gt;PostgreSQL database&lt;/a&gt;. PostgreSQL requires a persistent storage database provided by an external Network File System (NFS) server partition.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To follow the instructions in this article, you will need the following components in your development environment:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An OpenShift 4 or higher cluster with leader and follower nodes.&lt;/li&gt; &lt;li&gt;Red Hat’s single sign-on technology version 7.4, deployed on Red Hat OpenShift.&lt;/li&gt; &lt;li&gt;An external NFS server.&lt;/li&gt; &lt;/ul&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: For the purpose of this article, I will refer to leader and follower nodes, although the code output uses the terminology of &lt;code&gt;master&lt;/code&gt; and &lt;code&gt;worker&lt;/code&gt; nodes.&lt;/p&gt; &lt;h3&gt;Setting up the external NFS server&lt;/h3&gt; &lt;p&gt;NFS allows remote hosts to mount file systems over a network and interact with them as though they are mounted locally. This lets system administrators consolidate resources on centralized servers on the network. For an introduction to NFS concepts and fundamentals, see the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/ch-nfs"&gt;introduction to NFS&lt;/a&gt; in the &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux 7&lt;/a&gt; documentation.&lt;/p&gt; &lt;h3&gt;Creating persistent storage for an OpenShift 4.5 cluster&lt;/h3&gt; &lt;p&gt;OpenShift 4.5 doesn&amp;#8217;t provide persistent storage volumes out of the box. You can either map persistent storage manually or you can define it using the OpenShift control plane&amp;#8217;s Machine Config Operator. See &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/architecture/control-plane.html#architecture-machine-roles_control-plane"&gt;&lt;i&gt;Machine roles in OpenShift Container Platform&lt;/i&gt;&lt;/a&gt; for more about the &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/architecture/control-plane.html#understanding-machine-config-operator_control-plane"&gt;Machine Config Operator&lt;/a&gt;. In the next sections, I will show you how to map persistent storage manually.&lt;/p&gt; &lt;h2&gt;Mapping OpenShift persistent storage to NFS&lt;/h2&gt; &lt;p&gt;To access an NFS partition from an OpenShift cluster&amp;#8217;s follower (&lt;code&gt;worker&lt;/code&gt;) nodes, you must manually map persistent storage to the NFS partition. In this section, you will do the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Get a list of nodes.&lt;/li&gt; &lt;li&gt;Access a follower node: &lt;ul&gt; &lt;li&gt;Ping the NFS server.&lt;/li&gt; &lt;li&gt;Mount the exported NFS server partition.&lt;/li&gt; &lt;li&gt;Verify that the file is present on the NFS server.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Get a list of nodes&lt;/h3&gt; &lt;p&gt;To get a list of the nodes, enter:&lt;/p&gt; &lt;pre&gt;$ oc get nodes&lt;/pre&gt; &lt;p&gt;The list of requested nodes displays as follows:&lt;/p&gt; &lt;pre&gt;NAME STATUS ROLES AGE VERSION master-0.example.com Ready master 81m v1.18.3+2cf11e2 worker-0.example.com Ready worker 72m v1.18.3+2cf11e2 worker-1.example.com Ready worker 72m v1.18.3+2cf11e2 &lt;/pre&gt; &lt;h3&gt;Access the follower node&lt;/h3&gt; &lt;p&gt;To access the follower node, use the &lt;code&gt;oc debug node&lt;/code&gt; command and type &lt;code&gt;chroot /root&lt;/code&gt;, as shown here:&lt;/p&gt; &lt;pre&gt;$ oc debug node/worker-0.example.com Starting pod/worker-example.com-debug ... &lt;/pre&gt; &lt;p&gt;Run &lt;code&gt;chroot /host&lt;/code&gt; before issuing further commands:&lt;/p&gt; &lt;pre&gt;sh-4.2# chroot /host&lt;/pre&gt; &lt;h4&gt;Ping the NFS server&lt;/h4&gt; &lt;p&gt;Next, ping the NFS server from the follower node in debug mode:&lt;/p&gt; &lt;pre&gt;sh-4.2#ping node-0.nfserver1.example.com &lt;/pre&gt; &lt;h4&gt;Mount the NFS partition&lt;/h4&gt; &lt;p&gt;Now, mount the NFS partition from the follower node (still in debug mode):&lt;/p&gt; &lt;pre&gt;sh-4.2#mount node-0.nfserver1.example.com:/persistent_volume1 /mnt &lt;/pre&gt; &lt;h4&gt;Verify that the file is present on the NFS server&lt;/h4&gt; &lt;p&gt;Create a dummy file from the follower node in debug mode:&lt;/p&gt; &lt;pre&gt;sh-4.2#touch /mnt/test.txt &lt;/pre&gt; &lt;p&gt;Verify that the file is present on the NFS server:&lt;/p&gt; &lt;pre&gt;$ cd /persistent_volume1 &lt;/pre&gt; &lt;pre&gt;$ ls -al total 0 drwxrwxrwx. 2 root root 22 Sep 23 09:31 . dr-xr-xr-x. 19 root root 276 Sep 23 08:37 .. -rw-r--r--. 1 nfsnobody nfsnobody 0 Sep 23 09:31 test.txt &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You must issue the same command sequence for every follower node in the cluster.&lt;/p&gt; &lt;h2&gt;Persistent volume storage&lt;/h2&gt; &lt;p&gt;The previous section showed how to define and mount an NFS partition. Now, you&amp;#8217;ll use the NFS partition to define and map an OpenShift persistent volume. The steps are as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Make the persistent volume writable on the NFS server.&lt;/li&gt; &lt;li&gt;Map the persistent volume to the NFS partition.&lt;/li&gt; &lt;li&gt;Create the persistent volume.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Make the persistent volume writable&lt;/h3&gt; &lt;p&gt;Make the persistent volume writable on the NFS server:&lt;/p&gt; &lt;pre&gt;$ chmod 777 /persistent_volume1&lt;/pre&gt; &lt;h3&gt;Map the persistent volume to the NFS partition&lt;/h3&gt; &lt;p&gt;Define a storage class and specify the default storage class. For example, the following YAML defines the &lt;code&gt;StorageClass&lt;/code&gt; of &lt;code&gt;slow&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/no-provisioner reclaimPolicy: Delete &lt;/pre&gt; &lt;p&gt;Next, make the storage class the default class:&lt;/p&gt; &lt;pre&gt;$ oc create -f slow_sc.yaml  $ oc patch storageclass slow -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}' storageclass.storage.k8s.io/slow patched. &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;StorageClass&lt;/code&gt; is common to all namespaces.&lt;/p&gt; &lt;h3&gt;Create the persistent volume&lt;/h3&gt; &lt;p&gt;You can create a persistent volume either from the OpenShift  admin console or from a YAML file, as follows:&lt;/p&gt; &lt;pre&gt;apiVersion: v1 kind: PersistentVolume metadata: name: example spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: slow nfs: path: /persistent_volume2 server: node-0.nfserver1.example.com $ oc create -f pv.yaml persistentvolume/example created $ oc get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE example 5Gi RWO Retain Available slow 5s &lt;/pre&gt; &lt;h2&gt;Deploy SSO on the OpenShift cluster&lt;/h2&gt; &lt;p&gt;Next, you&amp;#8217;ll deploy Red Hat’s single sign-on technology on the OpenShift cluster. The steps are as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a new project.&lt;/li&gt; &lt;li&gt;Download the &lt;code&gt;sso-74&lt;/code&gt; templates.&lt;/li&gt; &lt;li&gt;Customize the &lt;code&gt;sso74-ocp4-x509-postgresql-persistent&lt;/code&gt; template.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Create a new project&lt;/h3&gt; &lt;p&gt;Create a new project using the &lt;code&gt;oc new-project&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt;$ oc new-project sso-74&lt;/pre&gt; &lt;p&gt;Import the OpenShift image for Red Hat’s single sign-on technology 7.4:&lt;/p&gt; &lt;pre&gt;$ oc -n openshift import-image rh-sso-7/sso74-openshift-rhel8:7.4 --from=registry.redhat.io/rh-sso-7/sso74-openshift-rhel8:7.4 --confirm &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you need to delete and re-create the SSO project,  first delete the secrets, which are project-specific.&lt;/p&gt; &lt;h3&gt;Download the sso-74 templates&lt;/h3&gt; &lt;p&gt;Here is the list of available templates:&lt;/p&gt; &lt;pre&gt;$ oc get templates -n openshift -o name | grep -o 'sso74.\+' sso74-https sso74-ocp4-x509-https sso74-ocp4-x509-postgresql-persistent sso74-postgresql sso74-postgresql-persistent &lt;/pre&gt; &lt;h3&gt;Customize the sso74-ocp4-x509-postgresql-persistent template&lt;/h3&gt; &lt;p&gt;Next, you&amp;#8217;ll customize the &lt;code&gt;sso74-ocp4-x509-postgresql-persistent&lt;/code&gt; template to allow a TLS connection to the persistent PostgreSQL database:&lt;/p&gt; &lt;pre&gt;$oc process sso74-ocp4-x509-postgresql-persistent -n openshift SSO_ADMIN_USERNAME=admin SSO_ADMIN_PASSWORD=password -o yaml &amp;#62; my_sso74-x509-postgresql-persistent.yaml &lt;/pre&gt; &lt;h2&gt;Control manually set pod replica scheduling&lt;/h2&gt; &lt;p&gt;You control manually set pod replica scheduling the first time it is used. Within the updated template file, &lt;code&gt;my_sso74-x509-postgresql-persistent.yaml&lt;/code&gt;, set both replicas for &lt;code&gt;sso&lt;/code&gt; and &lt;code&gt;so -postgresql&lt;/code&gt; to within the deployment config of &lt;code&gt;sso&lt;/code&gt; and &lt;code&gt;sso-postgresql&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Setting the replicas to zero (0) within each deployment config lets you manually control the initial pod rollout. If that&amp;#8217;s not enough, you can also increase the &lt;code&gt;initialDelaySeconds&lt;/code&gt; value for the liveness and readiness probes. Here is the updated deployment config of &lt;code&gt;sso&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;kind: DeploymentConfig metadata: labels: application: sso rhsso: 7.4.2.GA template: sso74-x509-postgresql-persistent name: sso spec: &lt;strong&gt;replicas: 0&lt;/strong&gt; selector: deploymentConfig: sso &lt;/pre&gt; &lt;p&gt;Here is the updated config for &lt;code&gt;sso-postgresql&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;metadata: labels: application: sso rhsso: 7.4.2.GA template: sso74-x509-postgresql-persistent name: sso-postgresql spec: &lt;strong&gt;replicas: 0&lt;/strong&gt; selector:my_sso74-ocp4-x509-postgresql-persistent.yaml deploymentConfig: sso-postgresql &lt;/pre&gt; &lt;h3&gt;Process the YAML template&lt;/h3&gt; &lt;p&gt;Use the &lt;code&gt;oc create&lt;/code&gt; command to process the YAML template:&lt;/p&gt; &lt;pre&gt;$ oc create -f my_sso74-x509-postgresql-persistent.yaml service/sso created service/sso-postgresql created service/sso-ping created route.route.openshift.io/sso created deploymentconfig.apps.openshift.io/sso created deploymentconfig.apps.openshift.io/sso-postgresql created persistentvolumeclaim/sso-postgresql-claim created &lt;/pre&gt; &lt;h3&gt;Upscale the sso-postgresql pod&lt;/h3&gt; &lt;p&gt;Use the &lt;code&gt;oc scale&lt;/code&gt; command to upscale the &lt;code&gt;sso-postgresql&lt;/code&gt; pod:&lt;/p&gt; &lt;pre&gt;$oc scale --replicas=1 dc/sso-postgresql &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;:  Wait until the PostgreSQL pod has reached a ready state of 1/1. This might take a couple of minutes.&lt;/p&gt; &lt;pre&gt;$ oc get pods NAME READY STATUS RESTARTS AGE sso-1-deploy 0/1 Completed 0 10m sso-postgresql-1-deploy 0/1 Completed 0 10m sso-postgresql-1-fzgf7 1/1 Running 0 3m46s &lt;/pre&gt; &lt;p&gt;When the &lt;code&gt;sso-postgresql&lt;/code&gt; pod starts correctly, it provides the following log output:&lt;/p&gt; &lt;pre&gt;pg_ctl -D /var/lib/pgsql/data/userdata -l logfile start waiting for server to start....2020-09-25 15:13:01.579 UTC [37] LOG: listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432" 2020-09-25 15:13:01.588 UTC [37] LOG: listening on Unix socket "/tmp/.s.PGSQL.5432" 2020-09-25 15:13:01.631 UTC [37] LOG: redirecting log output to logging collector process 2020-09-25 15:13:01.631 UTC [37] HINT: Future log output will appear in directory "log". done server started /var/run/postgresql:5432 - accepting connections =&amp;#62; sourcing /usr/share/container-scripts/postgresql/start/set_passwords.sh ... ALTER ROLE waiting for server to shut down.... done server stopped Starting server... 2020-09-25 15:13:06.147 UTC [1] LOG: listening on IPv4 address "0.0.0.0", port 5432 2020-09-25 15:13:06.147 UTC [1] LOG: listening on IPv6 address "::", port 5432 2020-09-25 15:13:06.157 UTC [1] LOG: listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432" 2020-09-25 15:13:06.164 UTC [1] LOG: listening on Unix socket "/tmp/.s.PGSQL.5432" 2020-09-25 15:13:06.206 UTC [1] LOG: redirecting log output to logging collector process 2020-09-25 15:13:06.206 UTC [1] HINT: Future log output will appear in directory "log". &lt;/pre&gt; &lt;h3&gt;Upscale the sso pod&lt;/h3&gt; &lt;p&gt;Use the &lt;code&gt;oc scale&lt;/code&gt; command to upscale the &lt;code&gt;sso&lt;/code&gt; pod as follows:&lt;/p&gt; &lt;pre&gt;$ oc scale --replicas=1 dc/sso deploymentconfig.apps.openshift.io/sso &lt;/pre&gt; &lt;p&gt;Next, use the &lt;code&gt;oc get pods&lt;/code&gt; command and get the SSO pod fully up and running. It reaches a ready state of &lt;code&gt;1/1&lt;/code&gt; as shown:&lt;/p&gt; &lt;pre&gt;$oc get pods NAME READY STATUS RESTARTS AGE sso-1-d45k2 1/1 Running 0 52m sso-1-deploy 0/1 Completed 0 63m sso-postgresql-1-deploy 0/1 Completed 0 63m sso-postgresql-1-fzgf7 1/1 Running 0 57m&lt;/pre&gt; &lt;h2&gt;Testing&lt;/h2&gt; &lt;p&gt;The testing &lt;code&gt;oc status&lt;/code&gt; command includes:&lt;/p&gt; &lt;pre&gt;$ oc status In project sso-74 on server https://api.example.com:6443 svc/sso-ping (headless):8888 https://sso-sso-74.apps.example.com (reencrypt) (svc/sso) dc/sso deploys openshift/sso74-openshift-rhel8:7.4 deployment #1 deployed about an hour ago - 1 pod &lt;/pre&gt; &lt;pre&gt;svc/sso-postgresql - 172.30.113.48:5432 dc/sso-postgresql deploys openshift/postgresql:10 deployment #1 deployed about an hour ago - 1 pod &lt;/pre&gt; &lt;p&gt;You can now directly access the OpenShift 4.5 platform using the single sign-on technology admin console at &lt;em&gt;https://sso-sso-74.apps.example.com&lt;/em&gt;. Log in with your admin password and credentials.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article highlighted the basic steps to be executed when deploying Red Hat&amp;#8217;s single sign-on technology 7.4 on OpenShift. Deploying SSO on Openshift makes OpenShift&amp;#8217;s SSO features available out of the box. As one example, it is very easy to increase your workload capacity by adding new single sign-on pods to your Openshift deployment during horizontal scaling.&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fintegrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift%2F&amp;#38;linkname=Integrate%20Red%20Hat%E2%80%99s%20single%20sign-on%20technology%207.4%20with%20Red%20Hat%20OpenShift" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fintegrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift%2F&amp;#38;linkname=Integrate%20Red%20Hat%E2%80%99s%20single%20sign-on%20technology%207.4%20with%20Red%20Hat%20OpenShift" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fintegrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift%2F&amp;#38;linkname=Integrate%20Red%20Hat%E2%80%99s%20single%20sign-on%20technology%207.4%20with%20Red%20Hat%20OpenShift" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fintegrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift%2F&amp;#38;linkname=Integrate%20Red%20Hat%E2%80%99s%20single%20sign-on%20technology%207.4%20with%20Red%20Hat%20OpenShift" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fintegrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift%2F&amp;#38;linkname=Integrate%20Red%20Hat%E2%80%99s%20single%20sign-on%20technology%207.4%20with%20Red%20Hat%20OpenShift" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fintegrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift%2F&amp;#38;linkname=Integrate%20Red%20Hat%E2%80%99s%20single%20sign-on%20technology%207.4%20with%20Red%20Hat%20OpenShift" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fintegrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift%2F&amp;#38;linkname=Integrate%20Red%20Hat%E2%80%99s%20single%20sign-on%20technology%207.4%20with%20Red%20Hat%20OpenShift" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fintegrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift%2F&amp;#038;title=Integrate%20Red%20Hat%E2%80%99s%20single%20sign-on%20technology%207.4%20with%20Red%20Hat%20OpenShift" data-a2a-url="https://developers.redhat.com/blog/2021/03/25/integrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift/" data-a2a-title="Integrate Red Hat’s single sign-on technology 7.4 with Red Hat OpenShift"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/25/integrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift/"&gt;Integrate Red Hat’s single sign-on technology 7.4 with Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/xq9iR3Qr2fw" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In this article, you will learn how to integrate Red Hat’s single sign-on technology 7.4 with Red Hat OpenShift 4. For this integration, we&amp;#8217;ll use the PostgreSQL database. PostgreSQL requires a persistent storage database provided by an external Network File System (NFS) server partition. Prerequisites To follow the instructions in this article, you will need [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/25/integrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift/"&gt;Integrate Red Hat’s single sign-on technology 7.4 with Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/25/integrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">791907</post-id><dc:creator>orivat</dc:creator><dc:date>2021-03-25T07:00:59Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/25/integrate-red-hats-single-sign-on-technology-7-4-with-red-hat-openshift/</feedburner:origLink></entry><entry><title>Db2 and Oracle connectors coming to Debezium 1.4 GA</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/NbtzS3-uoGU/" /><category term="Event-Driven" /><category term="Stream Processing" /><category term="debezium" /><category term="Debezium Db2" /><category term="Debezium Oracle" /><category term="kafka" /><category term="kafka connect" /><author><name>Hugo Guerrero</name></author><id>https://developers.redhat.com/blog/?p=884027</id><updated>2021-03-25T07:00:03Z</updated><published>2021-03-25T07:00:03Z</published><content type="html">&lt;p&gt;This article gives an overview of the new Red Hat Integration &lt;a target="_blank" rel="nofollow" href="/topics/event-driven/connectors"&gt;Debezium connectors&lt;/a&gt; and features included in Debezium 1.4&amp;#8217;s general availability (GA) release. Developers now have two more options for streaming data to &lt;a target="_blank" rel="nofollow" href="/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; from their datastores and a supported integration to handle data schemas.&lt;/p&gt; &lt;p&gt;The GA of the Db2 connector lets developers stream data from Db2. The Oracle connector, now in &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/support/offerings/techpreview/"&gt;technical preview&lt;/a&gt;, provides an easy way to capture changes from one of the most popular databases. Finally, developers can delegate the Debezium schema through the fully supported integration with Red Hat Integration’s service registry.&lt;/p&gt; &lt;h2&gt;What is Debezium?&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://debezium.io/"&gt;Debezium&lt;/a&gt; is a set of distributed services that captures row-level database changes so that applications can view and respond to them. Debezium connectors record all events to Kafka clusters managed by &lt;a href="https://developers.redhat.com/blog/2018/10/29/how-to-run-kafka-on-openshift-the-enterprise-kubernetes-with-amq-streams/"&gt;Red Hat AMQ Streams&lt;/a&gt;. Applications use &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/resources/amq-streams-datasheet"&gt;AMQ Streams&lt;/a&gt; to consume change events.&lt;/p&gt; &lt;p&gt;Debezium uses the &lt;a target="_blank" rel="nofollow" href="/blog/2020/02/14/using-secrets-in-apache-kafka-connect-configuration/"&gt;Apache Kafka Connect&lt;/a&gt; framework, which transforms Debezium’s connectors into Kafka Connector source connectors. They can be deployed and managed using Kafka Connect Kubernetes custom resources provided by AMQ Streams.&lt;/p&gt; &lt;p&gt;The latest 1.4 release introduces new connectors, advances others, and adds features to handle schemas efficiently.&lt;/p&gt; &lt;h2&gt;Debezium connector for Db2 goes GA&lt;/h2&gt; &lt;p&gt;Debezium’s implementation of SQL Server inspired its Db2 connector. The SQL Server connector is based on the &lt;a target="_blank" rel="nofollow" href="https://www.ibm.com/support/pages/q-replication-and-sql-replication-product-documentation-pdf-format-version-101-linux-unix-and-windows"&gt;Abstract Syntax Notation (ASN) capture and apply agents&lt;/a&gt; used for SQL replication in Db2. Db2 connector agents generate change data for tables that are in capture mode. They also monitor the tables and store change events for table updates. The Debezium connector then uses an SQL interface to query change-data tables for change events.&lt;/p&gt; &lt;p&gt;The Db2 connector is now GA after being &lt;a href="https://developers.redhat.com/blog/2020/11/05/capture-ibm-db2-data-changes-with-debezium-db2-connector/"&gt;available in technical preview&lt;/a&gt; for developers to try and provide feedback, as well as after extensive testing from the Red Hat team. The Db2 connector gives Db2 for Linux users a supported mechanism to stream their database. You can read more about the connector and its configuration in the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2021.q1/html/debezium_user_guide/debezium-connector-for-db2"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Debezium connector for Oracle Database in developer preview&lt;/h2&gt; &lt;p&gt;One of the most requested connector plug-ins is coming to Red Hat Integration. You can now stream your data from Oracle databases with the Debezium connector for Oracle, now in developer preview.&lt;/p&gt; &lt;p&gt;Oracle Database is a vital part of many organizations&amp;#8217; architectures, as developers created complete systems that used the database as their applications&amp;#8217; core. The data stored in Oracle databases is still critical for those organizations, and having access to it at the same time new applications unfold is essential for a successful migration to modern system architectures.&lt;/p&gt; &lt;p&gt;The Debezium connector for Oracle Database can monitor and record the row-level changes in databases on Oracle server version 12R2 and later. Debezium uses &lt;a target="_blank" rel="nofollow" href="https://docs.oracle.com/cd/E18283_01/server.112/e16536/logminer.htm"&gt;Oracle’s native LogMiner&lt;/a&gt; database package available as part of the Oracle Database utilities. LogMiner provides a way to query online and archived redo log files.&lt;/p&gt; &lt;p&gt;With the Debezium connector for Oracle, developers can stream data changes in their databases directly into &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;AMQ Streams Apache Kafka clusters&lt;/a&gt;. Streaming events allows anyone to get the best of their data using modern, hybrid cloud technology. Achieve &lt;a target="_blank" rel="nofollow" href="https://twitter.com/gunnarmorling/status/1123191912800845825"&gt;“liberation for your data.”&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Integration with Service Registry&lt;/h2&gt; &lt;p&gt;The Red Hat Integration &lt;a href="https://developers.redhat.com/blog/2020/12/09/new-features-and-storage-options-in-red-hat-integration-service-registry-1-1-ga/"&gt;service registry&lt;/a&gt; is a datastore for standard event schemas and API designs. As a developer, you can use it to decouple the structure of your data from your applications. You can also use it to share and manage your data structure using a REST interface. Red Hat’s service registry is built on the &lt;a target="_blank" rel="nofollow" href="https://www.apicur.io/registry/docs/apicurio-registry/1.3.3.Final/getting-started/assembly-intro-to-the-registry.html"&gt;Apicurio Registry&lt;/a&gt;, an open source community project.&lt;/p&gt; &lt;p&gt;Debezium uses a JSON converter to serialize record keys and values into JSON documents. By default, the JSON converter includes a record’s message schema, so each record is quite lengthy. Another option is to use other serialization formats, like &lt;a target="_blank" rel="nofollow" href="http://avro.apache.org/"&gt;Apache Avro&lt;/a&gt; or &lt;a target="_blank" rel="nofollow" href="https://developers.google.com/protocol-buffers"&gt;Google Protocol Buffers&lt;/a&gt;, to serialize and deserialize each record’s keys and values. If you want to use any of these formats for serialization, Service Registry manages the data format’s message schemas and versions. You can specify the desired converter in your Debezium connector configuration. The converter then maps Kafka Connect schemas to that data format’s schemas.&lt;/p&gt; &lt;p&gt;Previously offered as a technical preview, the integration between Debezium and Service Registry is now fully supported. You can check how it works in my &lt;a target="_blank" rel="nofollow" href="https://youtu.be/NtGF-kwvcFI"&gt;example video&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Get started with Debezium and Kafka&lt;/h2&gt; &lt;p&gt;You can download the Red Hat Integration Debezium connectors from the &lt;a href="https://developers.redhat.com/products"&gt;Red Hat Developer portal&lt;/a&gt;. You can also check out &lt;a href="https://developers.redhat.com/videos/youtube/QYbXDp4Vu-8/"&gt;Gunnar Morling’s webinar on Debezium and Kafka&lt;/a&gt; (February 2019) from the &lt;a href="https://developers.redhat.com/devnation/the-show"&gt;DevNation Tech Talks&lt;/a&gt; series, or his &lt;a target="_blank" rel="nofollow" href="https://www.infoq.com/presentations/data-streaming-kafka-debezium/"&gt;Kafka and Debezium presentation&lt;/a&gt; at QCon (January 2020). Finally, you can learn more about the various &lt;a href="https://developers.redhat.com/topics/event-driven/connectors"&gt;connectivity options&lt;/a&gt; available for your events on Red Hat Developer.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fdb2-and-oracle-connectors-coming-to-debezium-1-4-ga%2F&amp;#38;linkname=Db2%20and%20Oracle%20connectors%20coming%20to%20Debezium%201.4%20GA" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fdb2-and-oracle-connectors-coming-to-debezium-1-4-ga%2F&amp;#38;linkname=Db2%20and%20Oracle%20connectors%20coming%20to%20Debezium%201.4%20GA" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fdb2-and-oracle-connectors-coming-to-debezium-1-4-ga%2F&amp;#38;linkname=Db2%20and%20Oracle%20connectors%20coming%20to%20Debezium%201.4%20GA" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fdb2-and-oracle-connectors-coming-to-debezium-1-4-ga%2F&amp;#38;linkname=Db2%20and%20Oracle%20connectors%20coming%20to%20Debezium%201.4%20GA" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fdb2-and-oracle-connectors-coming-to-debezium-1-4-ga%2F&amp;#38;linkname=Db2%20and%20Oracle%20connectors%20coming%20to%20Debezium%201.4%20GA" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fdb2-and-oracle-connectors-coming-to-debezium-1-4-ga%2F&amp;#38;linkname=Db2%20and%20Oracle%20connectors%20coming%20to%20Debezium%201.4%20GA" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fdb2-and-oracle-connectors-coming-to-debezium-1-4-ga%2F&amp;#38;linkname=Db2%20and%20Oracle%20connectors%20coming%20to%20Debezium%201.4%20GA" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F25%2Fdb2-and-oracle-connectors-coming-to-debezium-1-4-ga%2F&amp;#038;title=Db2%20and%20Oracle%20connectors%20coming%20to%20Debezium%201.4%20GA" data-a2a-url="https://developers.redhat.com/blog/2021/03/25/db2-and-oracle-connectors-coming-to-debezium-1-4-ga/" data-a2a-title="Db2 and Oracle connectors coming to Debezium 1.4 GA"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/25/db2-and-oracle-connectors-coming-to-debezium-1-4-ga/"&gt;Db2 and Oracle connectors coming to Debezium 1.4 GA&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/NbtzS3-uoGU" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;This article gives an overview of the new Red Hat Integration Debezium connectors and features included in Debezium 1.4&amp;#8217;s general availability (GA) release. Developers now have two more options for streaming data to Apache Kafka from their datastores and a supported integration to handle data schemas. The GA of the Db2 connector lets developers stream [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/25/db2-and-oracle-connectors-coming-to-debezium-1-4-ga/"&gt;Db2 and Oracle connectors coming to Debezium 1.4 GA&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/25/db2-and-oracle-connectors-coming-to-debezium-1-4-ga/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">884027</post-id><dc:creator>Hugo Guerrero</dc:creator><dc:date>2021-03-25T07:00:03Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/25/db2-and-oracle-connectors-coming-to-debezium-1-4-ga/</feedburner:origLink></entry><entry><title type="html">Point of sale - Common architectural elements</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Xu-Ma63FqPQ/point-of-sale-common-architectural-elements.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/AEox-BKSr3I/point-of-sale-common-architectural-elements.html</id><updated>2021-03-25T06:00:00Z</updated><content type="html">Part 2 - Common architectural elements In  from this series I introduced a use case around point of sale imaging for retail stores. The process was laid out how I've approached the use case and how portfolio solutions are the base for researching a generic architectural blueprint.  The only thing left to cover was the order in which you'll be led through the blueprint details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. This will start our journey into the logical elements that make up the point of sale imaging architecture blueprint. BLUEPRINTS REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common blueprint that was uncovered researching those solutions. It's my intent to provide a blueprint that provides guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architectural blueprint, but I've chosen a format that I hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this blueprint and outline the solution. FROM SPECIFIC TO GENERIC Before diving in to the common elements, it might be nice to understand that this is not a catch all for every possible supply chain integration solution. It's a collection of identified elements that I've uncovered in multiple customer implementations. These elements presented here are then the generic common architectural elements that I've identified and collected in to the generic architectural blueprint.  It's my intent to provide a blueprint for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.  It's my job here to describe the architectural blueprint generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architectural blueprint. There are many ways to represent each element, but I've chosen some icons, text and colours that I hope are going to make it all easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in my research. POINT OF SALE Starting at the point of sale location you have a block of three elements to consider. The first listed here is the catalog maintained with available items for sale in the running inventory known as the SKU Catalog. This is going to be tied directly into the Retail Data Framework blueprint which is indirectly managing inventory and stock control through the Real-time Stock Control blueprint (be sure to check out these article series as they appear). Another important element on the point of sale end of this logical view is the sales data cache where all sales activities are collected and held here for sharing back to the retail organisation.  Finally, the point of sale application is on site in the store and the main focus of providing an end point application image pipeline in this architecture for use across an entire retail organisation. STORE SERVER Inside each store in the retail organisation can be found the store server, a part of the infrastructure that is hosting the elements needed to facilitate on site point of sale image pipelines and the daily management of communication, sales data, and stock control information. Starting at the top with the element SKU Catalog, not to be confused with the same element found on a point of sale component, is taking input from each of the point of sale stations in a store. This information is communicated back to the main retail organisations stock control mechanisms.  Next up, an image cache element is hosting the retail organisations centrally developed collection of point of sale images. They are collected here at the individual store server for use in updating and restoring in store point of sale devices.  The next three elements are all related to sales data. There is an element for sales data collection which is used to cache all the point of sale incoming sale information throughout the days sales activities. The sales data aggregation element is ensuring all data collected can be transported in the correct format. Finally, sales data integration element is tying together the communication points from the on site stores to the retail organisations central data locations. There can be a lot of different types of integration needs making this one of the most flexible elements in the architecture. The final element, application image cache, represents the collection of other not directly related to point of sale device images. Specific stores might have other application needs or make use of central retail organisation applications related to, for example, store health and safety (see the article series for Store Health and Safety blueprint as it appears). INFRASTRUCTURE SERVICES These elements in the common architecture were pretty consistent across all of the point of sale solutions examined. These tended to be core elements setup in the retail organisations central location with the ability to control communication for point of sale images, store applications, and sales data collection. The image automation element provides the core tooling for automating all aspects of their infrastructure delivery services. Together with their satellite server element, the retail organisation is capable of maintaining integrity of image distribution and application delivery across their store landscape. Sales data integration element is the retail central organisation side of sales data collection from all the stores providing sales information. POINT OF SALE CI / CD This collection of elements gives some insights into the central retail organisations development origins used to deliver all their point of sale images and in store application needs. A CI/CD platform is core to development, testing, packaging, and delivering point of sale images and store applications to the image registry. Core to this process is a source code management system (SCM). STORAGE SERVICES The storage services uncovered in this solution space was a fairly narrow single physical block storage element.  There was not a lot of need for diversity in these services as the solution is squarely focused on the infrastructure development and delivery pipeline. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture blueprint for the point of sale imaging use case.  An overview of this series on the point of sale portfolio architecture blueprint can be found here: 1. 2. 3. Example image distribution architecture Catch up on any articles you missed by following one of the links above. Next in this series, taking a look at an example image distribution architecture to provide you with a map for your own point of sale. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Xu-Ma63FqPQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/AEox-BKSr3I/point-of-sale-common-architectural-elements.html</feedburner:origLink></entry><entry><title type="html">This Week in JBoss - 25 March 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/PuGcVLOfcQ4/weekly-2021-03-25.html" /><category term="news" /><category term="weekly_update" /><category term="weekly_editorial" /><category term="wildfly" /><category term="vertx" /><category term="quarkus" /><category term="kie" /><category term="camel" /><category term="kogito" /><author><name>Kevin Conner</name><uri>https://www.jboss.org/people/kevin-conner</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-03-25.html</id><updated>2021-03-25T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="news, weekly_update, weekly_editorial, wildfly, vertx, quarkus, kie, camel, kogito"&gt; &lt;h1&gt;This Week in JBoss - 25 March 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Welcome to another edition of the JBoss Editorial where we search through the JBoss Community for interesting articles and updates&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_introducing_the_prolific_eric_schabell"&gt;Introducing the prolific Eric Schabell&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;If you are a regular reader of the JBoss Editorial you will be very familiar with the work of Eric Schabell, arguably one of the most prolific writers within the JBoss Community. This edition is no different and sees Eric product three more articles.&lt;/p&gt; &lt;p&gt;Eric’s first article is a continuation of his Supply Chain Integration Blueprint discussion. In this article Eric &lt;a href="https://www.schabell.org/2021/03/supply-chain-integration-example-store-integration-architecture.html"&gt;expands on the common architectural elements discussed previously and outlines an example Store integration architecture for Supply Chain integration&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Eric’s second article walks us through &lt;a href="https://www.schabell.org/2021/03/codeready-containers-howto-setup-openshift-47-on-local-machine.html"&gt;the necessary steps for installing and running the OpenShift Container Platform 4.7 on our local machines&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Eric’s final article kicks off a new series of articles within which he will introduce a new architectural blueprint covering &lt;a href="https://www.schabell.org/2021/03/point-of-sale-an-architectural-introduction.html"&gt;"Simplifying and modernizing central management of distributed point-of-sale devices with built in support for container based applications"&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_whats_new_in_kogito_land"&gt;What’s new in Kogito land?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;The &lt;a href="http://kie.org"&gt;Kogito team&lt;/a&gt; have been busy over the last few weeks publishing a number of interesting blog posts covering different aspects of the Kogito ecosystem.&lt;/p&gt; &lt;p&gt;We begin our coverage with an article written by William &lt;a href="https://blog.kie.org/2021/03/building-prometheus-dashboards-in-business-central.html"&gt;demonstrating how to integrate metrics from a Prometheus datasource into the Business Central&lt;/a&gt;. The article walks you through the process from the creation of the data set within Dashbuilder, building the dashboard, how to consider filtering requirements and how to test the dashboard.&lt;/p&gt; &lt;p&gt;The next article, written by Manaswini, follows on from William’s article and &lt;a href="https://blog.kie.org/2021/03/time-series-component-for-dashbuilder.html"&gt;demonstrates how you can further extend your dashboard through the integration and visualisation of time series data&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The final article, written by Ricardo, introduces us to Kogito’s support for the &lt;a href="https://github.com/serverlessworkflow/specification/"&gt;CNCF Serverless Workflow&lt;/a&gt; specification, &lt;a href="https://blog.kie.org/2021/03/restful-services-orchestration-with-kogito-and-openapi.html"&gt;allowing Kogito to provide a serverless interface to a service which can also be used to orchestrate RESTful services described through the OpenAPI standard&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_clustering_wildfly_using_dns_ping"&gt;Clustering WildFly using DNS ping&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;When running within a kubernetes environment it is natural to consider the &lt;a href="http://www.jgroups.org/manual5/index.html#_kube_ping"&gt;KUBE_PING protocol&lt;/a&gt; for cluster discovery, a protocol relying on the ability to query the kubernetes API server. If you want to remain agnostic of your environment, or if you do not have sufficient permissions to query the kubernetes API server, then an alternative approach to consider would be the &lt;a href="https://www.wildfly.org//news/2021/03/12/Bootable-jar-jkube-clustering-dns-ping-openshift/"&gt;DNS_PING protocol&lt;/a&gt; as &lt;a href="https://www.wildfly.org//news/2021/03/12/Bootable-jar-jkube-clustering-dns-ping-openshift/"&gt;demonstrated by Yeray using a simple WildFly bootable JAR application&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_optimizing_camel_3_9_no_more_saw_tooth_jvm_garbage_collection"&gt;Optimizing Camel 3.9, no more saw tooth JVM garbage collection&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;The upcoming Camel 3.9 release will include some additional, significant improvements in the performance of the internal routing engine leading to a dramatic reduction in object allocation during continuous use. The Camel team focussed on the five major areas of the codebase they had identified during performance testing with the intention of reducing the allocation count as much as possible, they managed to go one better and achieved a sustained object allocation count of zero! For more information take a look at &lt;a href="http://www.davsclaus.com/2021/03/apache-camel-39-no-more-saw-tooth-jvm.html"&gt;Claus' blog describing the effort&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_monitoring_quarkus_applications_on_openshift"&gt;Monitoring Quarkus applications on OpenShift&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;If you are running a Quarkus application on OpenShift you will most likely have a need for integrating with the centralised prometheus instance within the cluster. This integration is fairly straight forward and can be achieved after following only a few simple steps &lt;a href="https://quarkus.io/blog/micrometer-prometheus-openshift/"&gt;as ably demonstrated by Jose&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_new_releases"&gt;New Releases&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;The &lt;a href="https://www.wildfly.org/"&gt;WildFly team&lt;/a&gt; have announced&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;the &lt;a href="https://www.wildfly.org/news/2021/03/11/WildFly-MicroProfile-Reactive-specifications-feature-pack-2.0/"&gt;2.0.0.Final release of the MicroProfile Reactive Specifications Feature Pack for WildFly&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;the &lt;a href="https://www.wildfly.org/news/2021/03/19/bootable-jar-4.0.Final-Released/"&gt;4.0.0.Final release of the WildFly Bootable JAR Maven Plugin&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;the &lt;a href="https://www.wildfly.org//news/2021/03/18/WildFly-s2i-23-Released/"&gt;23.0 release of the WildFly S2I images&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The &lt;a href="http://kie.org/"&gt;Kogito team&lt;/a&gt; have announced&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;the release of &lt;a href="https://blog.kie.org/2021/03/kogito-tooling-0-8-5-released.html"&gt;Kogito Tooling 0.8.5&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;the preliminary release of their &lt;a href="https://blog.kie.org/2021/03/scorecard-editor.html"&gt;PMML (4.4) Scorecard Editor&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The &lt;a href="https://vertx.io"&gt;Vert.x team&lt;/a&gt; have announced the release of &lt;a href="https://vertx.io/blog/eclipse-vert-x-4-0-3/"&gt;Eclipse Vert.x 4.0.3&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;That’s all for this edition of the JBoss Editorial, please join us in a couple of weeks when we will bring you more news and articles from the JBoss Community Projects.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/kevin-conner.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Kevin Conner&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/PuGcVLOfcQ4" height="1" width="1" alt=""/&gt;</content><dc:creator>Kevin Conner</dc:creator><feedburner:origLink>https://www.jboss.org/posts/weekly-2021-03-25.html</feedburner:origLink></entry><entry><title>JBoss Tools 4.19.0.AM1 for Eclipse 2021-03</title><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/jbossbuzz/~3/5pTwpYJxVHg/4.19.0.am1.html" /><category term="release" /><category term="jbosstools" /><category term="devstudio" /><category term="jbosscentral" /><author><name>jeffmaury</name></author><id>https://tools.jboss.org/blog/4.19.0.am1.html</id><updated>2021-03-26T14:26:10Z</updated><published>2021-03-24T00:00:00Z</published><content type="html">&lt;div&gt;&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Happy to announce 4.19.0.AM1 (Developer Milestone 1) build for Eclipse 2021-03.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Downloads available at &lt;a href="https://tools.jboss.org/downloads/jbosstools/2021-03/4.19.0.AM1.html"&gt;JBoss Tools 4.19.0 AM1&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="what-is-new"&gt;&lt;a class="anchor" href="#what-is-new"&gt;&lt;/a&gt;What is New?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Full info is at &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.19.0.AM1.html"&gt;this page&lt;/a&gt;. Some highlights are below.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="openshift"&gt;&lt;a class="anchor" href="#openshift"&gt;&lt;/a&gt;OpenShift&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="browser-based-login-to-an-openshift-cluster"&gt;&lt;a class="anchor" href="#browser-based-login-to-an-openshift-cluster"&gt;&lt;/a&gt;Browser based login to an OpenShift cluster&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;When it comes to login to a cluster, OpenShift Tools supported two different authentication mechanisms:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;user/password&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;token&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The drawback is that it does not cover clusters where a more enhanced and modern authentication infrastructure is in place. So it is now possible to login to the cluster through an embedded web browser.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In order to use it, go to the Login context menu from the Application Explorer view:&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="https://tools.jboss.org/documentation/whatsnew/openshift/images/weblogin1.png" alt="weblogin1" width="600" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Click on the &lt;code&gt;Retrieve token&lt;/code&gt; button and an embedded web browser will be displayed:&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="https://tools.jboss.org/documentation/whatsnew/openshift/images/weblogin2.png" alt="weblogin2" width="600" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Complete the workflow until you see a page that contains &lt;code&gt;Display Token&lt;/code&gt;:&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="https://tools.jboss.org/documentation/whatsnew/openshift/images/weblogin3.png" alt="weblogin3" width="600" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Click on &lt;code&gt;Display Token&lt;/code&gt;:&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The web browser is automatically closed and you’ll notice that the retrieved token has been set in the original dialog:&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="https://tools.jboss.org/documentation/whatsnew/openshift/images/weblogin4.png" alt="weblogin4" width="600" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect3"&gt; &lt;h4 id="devfile-registries-management"&gt;&lt;a class="anchor" href="#devfile-registries-management"&gt;&lt;/a&gt;Devfile registries management&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Since JBoss Tools 4.18.0.Final, the preferred way of developing components is now based on devfile, which is a YAML file that describe how to build the component and if required, launch other containers with other containers. When you create a component, you need to specify a devfile that describe your component. So either you component source contains its own devfile or you need to pick a devfile that is related to your component. In the second case, OpenShift Tools supports devfile registries that contains a set of different devfiles. There is a default registry (&lt;a href="https://github.com/odo-devfiles/registry" class="bare"&gt;https://github.com/odo-devfiles/registry&lt;/a&gt;) but you may want to have your own registries. It is now possible to add and remove registries as you want.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The registries are displayed in the OpenShift Application Explorer under the &lt;code&gt;Devfile registries&lt;/code&gt; node:&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="https://tools.jboss.org/documentation/whatsnew/openshift/images/registries1.png" alt="registries1" width="600" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Please note that expanding the registry node will list all devfiles from that registry with a description:&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="https://tools.jboss.org/documentation/whatsnew/openshift/images/registries2.png" alt="registries2" width="600" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A context menu on the &lt;code&gt;Devfile registries&lt;/code&gt; node allows you to add new registries, and on the registry node to delete it.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect3"&gt; &lt;h4 id="devfile-enhanced-editing-experience"&gt;&lt;a class="anchor" href="#devfile-enhanced-editing-experience"&gt;&lt;/a&gt;Devfile enhanced editing experience&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Although devfile registries can provide ready-to-use devfiles, there may be some advanced cases where users need to write their own devfile. As the syntax is quite complex, the YAML editor has been completed so that to provide:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;syntax validation&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;content assist&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect3"&gt; &lt;h4 id="support-for-python-based-components"&gt;&lt;a class="anchor" href="#support-for-python-based-components"&gt;&lt;/a&gt;Support for Python based components&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Python-based components were supported but debugging was not possible. This release brings integration between the Eclipse debugger and the Python runtime.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="hibernate-tools"&gt;&lt;a class="anchor" href="#hibernate-tools"&gt;&lt;/a&gt;Hibernate Tools&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A number of additions and updates have been performed on the available Hibernate runtime providers.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect3"&gt; &lt;h4 id="runtime-provider-updates"&gt;&lt;a class="anchor" href="#runtime-provider-updates"&gt;&lt;/a&gt;Runtime Provider Updates&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Hibernate 5.4 runtime provider now incorporates Hibernate Core version 5.4.29.Final and Hibernate Tools version 5.4.29a.Final.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="server-tools"&gt;&lt;a class="anchor" href="#server-tools"&gt;&lt;/a&gt;Server Tools&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="wildfly-23-server-adapter"&gt;&lt;a class="anchor" href="#wildfly-23-server-adapter"&gt;&lt;/a&gt;Wildfly 23 Server Adapter&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A server adapter has been added to work with Wildfly 23.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect3"&gt; &lt;h4 id="eap-7-4-beta-server-adapter"&gt;&lt;a class="anchor" href="#eap-7-4-beta-server-adapter"&gt;&lt;/a&gt;EAP 7.4 Beta Server Adapter&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The server adapter has been adapted to work with EAP 7.4 Beta.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Jeff Maury&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/5pTwpYJxVHg" height="1" width="1" alt=""/&gt;</content><summary>Happy to announce 4.19.0.AM1 (Developer Milestone 1) build for Eclipse 2021-03. Downloads available at JBoss Tools 4.19.0 AM1. What is New? Full info is at this page. Some highlights are below. OpenShift Browser based login to an OpenShift cluster When it comes to login to a cluster, OpenShift Tools supported two different authentication mechanisms: user/password token The drawback is that it does not cover clusters where a more enhanced and modern authentication infrastructure is in place. So it is now possible to login to the cluster through an embedded web browser. In order to use it, go to the Login context menu from the Application Explorer view: Click on the Retrieve...</summary><dc:creator>jeffmaury</dc:creator><dc:date>2021-03-24T00:00:00Z</dc:date><feedburner:origLink>https://tools.jboss.org/blog/4.19.0.am1.html</feedburner:origLink></entry><entry><title type="html">NEW model-specific DMN kie-server endpoints</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/U6NcGFtkqiQ/new-model-specific-dmn-kie-server-endpoints.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2021/03/new-model-specific-dmn-kie-server-endpoints.html</id><updated>2021-03-23T09:22:16Z</updated><content type="html">Modernising kie-server with new and more user-friendly DMN endpoints, better Swagger/OpenAPI documentation, easier JSON-based REST invocations; an intermediate step to help developers transitioning to service-oriented deployments such as a Kogito-based application. IN A NUTSHELL: The current DMN kie-server endpoints are fully compliant with kie-server extension design architecture, and aligned with all other kie-server services and extensions; however, some aspects of the current generic approach of kie-server sometimes are not very user-friendly for DMN evaluations, due to limitations of swagger documentation and the REST payloads requirements to follow the generic kie-server marshaller protocol. These aspects do apply to all kie-server services, including naturally DMN kie-server endpoints as well. On other hand, experience shown that building manually the REST payload on Kogito for DMN evaluation is very easy for end-users, thanks to key features pertaining to DMN core capabilities. This extends DMN on kie-server with new endpoints, leveraging those core capabilities; the new DMN endpoints provide better Swagger documentation and can be more easily consumed by end-users, therefore contributing to modernising the kie-server platform while also making easier to eventually transition to a full Kogito-based application and deployment! WHY IS THIS NEEDED? Currently on kie-server, the DMN service exposes 2 endpoints which are fully compliant with kie-server extension design architecture: * GET /server/containers/{containerId}/dmn Retrieves DMN model for given container * POST /server/containers/{containerId}/dmn Evaluates decisions for given input The current swagger documentation is agnostic to the actual model content of the knowledge asset, like for any other kie-server extension: This limited style of swagger documentation is sometimes an undesirable side-effect to the generic approach of kie-server extension design: * all kie-server extensions receive as input a generic String, which is actually converted internally to the extension using the generic kie-server marshaller. This causes the swagger documentation to not display anything meaningful for the request body besides Model==string, and the only helpful information can only be provided as a comment (“DMN context to be used while evaluation decisions as DMNContextKS type”). * all kie-server extensions return as output a ServiceResponse&lt;T&gt;, where the Java’s generic T is extension-specific. Generating swagger documentation with Java generics is already limited, in this case the difficulty compounds because the actual content of T varies, by DMN model to model ! * the DMN evaluation payload itself contains the coordinates of the model to be evaluated and the model-specific input context, per the original implementation requirements; but this interconnection between model coordinates values and input content structure, is pragmatically impossible to be defined meaningfully with a Swagger or OpenAPI descriptor. About the last point specifically, consider this example DMN payload: { "model-namespace": "https://kiegroup.org/dmn/_FA9849E2-C92E-4E27-83BF-07A7428DC9C9", "model-name": "Traffic Violation", "dmn-context": { "Driver": ..., "Violation": ... } } because the content of dmn-context depends on the values of model-namespace and model-name coordinates, there is no pragmatic way to define with Swagger/OpenAPI that dmn-context must have the properties “Driver”, “Violation” for this traffic violation model, or property “Customer” for another DMN model. Besides endpoint documentation limitations, experience proved that building manually from scratch the kie-server generic payload following the style of the kie-server generic marshaller is very difficult for most end-users (in fact we always advise to use the Kie Server Client API first, and not start from scratch, but this suggestion is often ignored anyway): * XML/JAXB format requires domain model pojo to be correctly annotated first, and building Java collection manually is quite tricky. * XML/XStream is a more natural format, still requires domain model pojo annotations, requires to respect the domain object FQN, but is yet another xml format while most end-users seem to prefer json instead. * JSON/Jackson would be the user preference nowadays, but requires to respect the domain object FQN which is very alien to json native users. Example. The correct way to marshall for Traffic Violation example, respecting the domain model defined in the kjar project, would be: { "model-namespace": "https://kiegroup.org/dmn/_FA9849E2-C92E-4E27-83BF-07A7428DC9C9", "model-name": "Traffic Violation", "dmn-context": { "Driver": { "com.acme.Driver" : { "Points": 15 }}, "Violation": { "com.acme.Violation" : { "Type": "speed", "Date": "2020-10-01", "Actual Speed": 111, "Speed Limit": 100 }} } } Everything would be much more easier, while building the JSON body payload manually for DMN evaluation, if we could drop the strict requirement to respect the generic kie-server marshalling format. NEW MODEL-SPECIFIC DMN KIE-SERVER ENDPOINTS We can now move past and beyond these limitations, thanks to the next generation of DMN endpoints on kie-server, leveraging some new DMN core capabilities: * programmatic generation of Swagger and OpenAPI (Swagger/OAS) metadata () * consistent DMNContext build from JSON, based on DMN Model metadata () to ultimately offer more user-friendly endpoints on kie-server for DMN evaluation! Following similar style to what is offered today via Kogito, summarized in this , we implemented the following new DMN endpoints on kie-server: 1. GET /server/containers/{containerId}/dmn/openapi.json (|.yaml) Retrieves Swagger/OAS for the DMN models in the kjar project 2. GET /server/containers/{containerId}/dmn/models/{modelname} Standard DMN XML but without any decision logic, so this can be used as a descriptor of the DMN model (which are the inputs, which are the decisions), while using the same format of the DMN XSD instead. 3. POST /server/containers/{containerId}/dmn/models/{modelname} JSON-only evaluation of a specific DMN model with a body payload tailored for the specific model 4. POST /server/containers/{containerId}/dmn/models/{modelname}/{decisionServiceName} JSON-only evaluation of a specific decision service of a specific DMN model with a body payload tailored for the specific model 5. POST /server/containers/{containerId}/dmn/models/{modelname}/dmnresult JSON-only evaluation of a specific DMN model with a body payload tailored for the specific model, but returning a JSON representation as a DMNResult 6. POST /server/containers/{containerId}/dmn/models/{modelname}/{decisionServiceName}/dmnresult JSON-only evaluation of a specific decision service of a specific DMN model with a body payload tailored for the specific model, but returning a JSON representation as a DMNResult For the difference between “business-domain” and “dmnresult” variants of the rest endpoints, as also linked above. Making reference to the Traffic Violation example model, this new capability can now offer on kie-server something similar to: As we can see, both the input body payload and the response body payload offer Swagger/OAS schemas which are consistent with the specific DMN model! This is possible thanks to a convergence of factors: * Because each REST POST endpoint for DMN evaluation is specific for DMN model in the REST Path, it is possible to offer Swagger/OAS definition which are DMN model-specific e.g.: because POST /server/containers/mykjar-project/dmn/traffic-violation is a REST endpoint specific to the Traffic Violation model, both its input and output payload can now be documented properly in the Swagger/OAS schema definitions. * Because each Swagger/OAS definition is offered at kjar/kie-container level, it is possible to generate programmatically the schema definitions for the DMN models contained only in the specific container. e.g.: because GET /server/containers/mykjar-project/dmn/openapi.json would offer only definitions for the DMN models inside “mykjar-project”. This is thanks to the following DMN core capability: programmatic generation of Swagger/OAS metadata () * Because these endpoints are DMN evaluation specific and focusing on a natural and idiomatic JSON usage, they do NOT require to follow the generic kie-server marshalling format. This is thanks to the following DMN core capability: consistent DMNContext build from JSON based on DMNModel metadata () ANY LIMITATIONS? Being a new set of endpoints, in addition to the currently existing ones, there is basically no impact on the already-existing DMN kie-server capabilities. As this proposed set of new endpoints are contained within a specific {containerId}, it also means that the openapi.json|.yaml swagger/OAS definition file is only kie-container specific. In turn, it means when accessing the swagger-ui client editor, user need to manually point to the container URL, for example something like: Finally, as this core capability do leverage Eclipse MicroProfile for OpenAPI Specification (OAS) and SmallRye-openapi-core, this requires making use of Swagger-UI and clients which are compatible with OpenAPI Specification version 3.0.3, onwards. CONCLUSIONS We believe this feature meaningfully extends the current set of capabilities, by providing more user-friendly DMN endpoints on kie-server! Developers can make full use of this new feature to simplify existing REST call invocations, and as a stepping stone to eventually migrate to a Kogito-based application. Have you tried it yet? Do you have any feedback? Let us know in the comments below! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/U6NcGFtkqiQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Matteo Mortari</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/new-model-specific-dmn-kie-server-endpoints.html</feedburner:origLink></entry><entry><title>What’s coming for Node.js developers at NearForm event</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/302eXEsoNtg/" /><category term="JavaScript" /><category term="Kubernetes" /><category term="Node.js" /><category term="Kubernetes JavaScript" /><category term="Kubernetes Node.js" /><category term="observability" /><author><name>Michael Dawson</name></author><id>https://developers.redhat.com/blog/?p=883127</id><updated>2021-03-23T07:00:34Z</updated><published>2021-03-23T07:00:34Z</published><content type="html">&lt;p&gt;Red Hat is sponsoring the very first &lt;a target="_blank" rel="nofollow" href="https://www.nearform.com/events/"&gt;NearForm Presents&lt;/a&gt; event on Mar. 31, hosted by IBM. This online event will feature four talks on interesting topics related to Node.js Core, along with exciting workshop options.&lt;/p&gt; &lt;p&gt;Our &lt;a target="_blank" rel="nofollow" href="https://github.com/nodejs"&gt;Node.js&lt;/a&gt; team is very active in the Node.js project and will present great content in these two talks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Consuming New Node.js Observability Features in Kubernetes Environments (&lt;a href="https://developers.redhat.com/blog/author/aalykiot/"&gt;Alex Alykiotis&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/author/lholmqui/"&gt;Luke Holmquist&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;What’s Next, the Future of Node.js (&lt;a href="https://developers.redhat.com/blog/author/bgriggs/"&gt;Bethany Griggs&lt;/a&gt;, Joe Sepi, and &lt;a href="https://developers.redhat.com/blog/author/midawson/"&gt;Michael Dawson&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Additionally, &lt;a href="https://developers.redhat.com/blog/author/jlord/"&gt;Joel Lord&lt;/a&gt; will host a lab on Apr. 1 to help &lt;a target="_blank" rel="nofollow" href="/topics/javascript"&gt;JavaScript&lt;/a&gt; developers ramp up on &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;: Kubernetes for JS developers.&lt;/p&gt; &lt;p&gt;We hope to see you there to talk about Node.js and everything that goes with it. You can &lt;a target="_blank" rel="nofollow" href="https://ti.to/nearform/nearform-presents-nodecore-broken-promises"&gt;register for the event&lt;/a&gt; online.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, check out &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;our Node.js page here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#038;title=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" data-a2a-url="https://developers.redhat.com/blog/2021/03/23/whats-coming-for-node-js-developers-at-nearform-event/" data-a2a-title="What’s coming for Node.js developers at NearForm event"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/23/whats-coming-for-node-js-developers-at-nearform-event/"&gt;What&amp;#8217;s coming for Node.js developers at NearForm event&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/302eXEsoNtg" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Red Hat is sponsoring the very first NearForm Presents event on Mar. 31, hosted by IBM. This online event will feature four talks on interesting topics related to Node.js Core, along with exciting workshop options. Our Node.js team is very active in the Node.js project and will present great content in these two talks: Consuming [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/23/whats-coming-for-node-js-developers-at-nearform-event/"&gt;What&amp;#8217;s coming for Node.js developers at NearForm event&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/23/whats-coming-for-node-js-developers-at-nearform-event/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">883127</post-id><dc:creator>Michael Dawson</dc:creator><dc:date>2021-03-23T07:00:34Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/23/whats-coming-for-node-js-developers-at-nearform-event/</feedburner:origLink></entry><entry><title>Monitor Node.js applications on Red Hat OpenShift with Prometheus</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Lh6Nke4fASw/" /><category term="Containers" /><category term="JavaScript" /><category term="Kubernetes" /><category term="Node.js" /><category term="Performance" /><category term="express.js" /><category term="monitoring" /><category term="openshift" /><category term="prometheus" /><author><name>Alexandros Alykiotis</name></author><id>https://developers.redhat.com/blog/?p=876397</id><updated>2021-03-22T07:00:23Z</updated><published>2021-03-22T07:00:23Z</published><content type="html">&lt;p&gt;A great thing about &lt;a target="_blank" rel="nofollow" href="/topics/nodejs-develop-server-side-javascript-applications"&gt;Node.js&lt;/a&gt; is how well it performs inside a container. With the shift to &lt;a target="_blank" rel="nofollow" href="/topics/containers"&gt;containerized deployments and environments&lt;/a&gt; comes extra complexity. One such complexity is observing what’s going on within your application and its resources, and when resource use is outside of the expected norms.&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt; is a tool that developers can use to increase observability. It is an installable service that gathers instrumentation metrics from your applications and stores them as time-series data. Prometheus is advanced and battle-tested, and a great option for Node.js applications running inside of a container.&lt;/p&gt; &lt;h2&gt;Default and custom instrumentation&lt;/h2&gt; &lt;p&gt;For your application to feed metrics to Prometheus, it must expose a metrics endpoint. For a Node.js application, the best way to expose the metrics endpoint is to use the &lt;a target="_blank" rel="nofollow" href="https://www.npmjs.com/package/prom-client"&gt;prom-client module&lt;/a&gt; available from the Node Package Manager (NPM) registry. The &lt;code&gt;prom-client&lt;/code&gt; module exposes all of the &lt;a target="_blank" rel="nofollow" href="https://prometheus.io/docs/instrumenting/writing_clientlibs/#standard-and-runtime-collectors"&gt;default metrics&lt;/a&gt; recommended by Prometheus.&lt;/p&gt; &lt;p&gt;The defaults include metrics such as &lt;code&gt;process_cpu_seconds_total&lt;/code&gt; and &lt;code&gt;process_heap_bytes&lt;/code&gt;. In addition to exposing default metrics, &lt;code&gt;prom-client&lt;/code&gt; allows developers to define their own metrics, as we&amp;#8217;ll do in this article.&lt;/p&gt; &lt;h2&gt;A simple Express.js app&lt;/h2&gt; &lt;p&gt;Let’s start by creating a simple &lt;a target="_blank" rel="nofollow" href="https://expressjs.com/"&gt;Express.js&lt;/a&gt; application. In this application, a service endpoint at &lt;code&gt;/api/greeting&lt;/code&gt; accepts &lt;code&gt;GET&lt;/code&gt; requests and returns a greeting as JSON. The following commands will get your project started:&lt;/p&gt; &lt;pre&gt;$ mkdir my-app &amp;#38;&amp;#38; cd my-app $ npm init -y $ npm i express body-parser prom-client &lt;/pre&gt; &lt;p&gt;This sequence of commands should create a &lt;code&gt;package.json&lt;/code&gt; file and install all of the application dependencies. Next, open the &lt;code&gt;package.json&lt;/code&gt; file in a text editor and add the following to the &lt;code&gt;scripts&lt;/code&gt; section:&lt;/p&gt; &lt;pre&gt;"start": "node app.js" &lt;/pre&gt; &lt;h3&gt;Application source code&lt;/h3&gt; &lt;p&gt;The following code is a fairly simple Express.js application. Create a new file in your text editor called &lt;code&gt;app.js&lt;/code&gt; and paste the following into it:&lt;/p&gt; &lt;pre&gt; 'use strict'; const express = require('express'); const bodyParser = require('body-parser'); // Use the prom-client module to expose our metrics to Prometheus const client = require('prom-client'); // enable prom-client to expose default application metrics const collectDefaultMetrics = client.collectDefaultMetrics; // define a custom prefix string for application metrics collectDefaultMetrics({ prefix: 'my_app:' }); const histogram = new client.Histogram({ name: 'http_request_duration_seconds', help: 'Duration of HTTP requests in seconds histogram', labelNames: ['method', 'handler', 'code'], buckets: [0.1, 5, 15, 50, 100, 500], }); const app = express(); const port = process.argv[2] || 8080; let failureCounter = 0; app.use(bodyParser.json()); app.use(bodyParser.urlencoded({ extended: true })); app.get('/api/greeting', async (req, res) =&gt; { const end = histogram.startTimer(); const name = req.query?.name || 'World'; try { const result = await somethingThatCouldFail(`Hello, ${name}`); res.send({ message: result }); } catch (err) { res.status(500).send({ error: err.toString() }); } res.on('finish', () =&gt; end({ method: req.method, handler: new URL(req.url, `http://${req.hostname}`).pathname, code: res.statusCode, }) ); }); // expose our metrics at the default URL for Prometheus app.get('/metrics', async (req, res) =&gt; { res.set('Content-Type', client.register.contentType); res.send(await client.register.metrics()); }); app.listen(port, () =&gt; console.log(`Express app listening on port ${port}!`)); function somethingThatCouldFail(echo) { if (Date.now() % 5 === 0) { return Promise.reject(`Random failure ${++failureCounter}`); } else { return Promise.resolve(echo); } } &lt;/pre&gt; &lt;h3&gt;Deploy the application&lt;/h3&gt; &lt;p&gt;You can use the following command to deploy the application to Red Hat OpenShift:&lt;/p&gt; &lt;pre&gt;$ npx nodeshift --expose &lt;/pre&gt; &lt;p&gt;This command creates all the OpenShift objects that your application needs in order to be deployed. After the deployment succeeds, you will be able to visit your application.&lt;/p&gt; &lt;h3&gt;Verify the application&lt;/h3&gt; &lt;p&gt;This application exposes two endpoints: &lt;code&gt;/api/greetings&lt;/code&gt; to get the greeting message and &lt;code&gt;/metrics&lt;/code&gt; to get the Prometheus metrics. First, you&amp;#8217;ll see the JSON greeting produced by visiting the &lt;code&gt;greetings&lt;/code&gt; URL:&lt;/p&gt; &lt;pre&gt;$ curl http://my-app-nodeshift.apps.ci-ln-5sqydqb-f76d1.origin-ci-int-gce.dev.openshift.com/api/greeting &lt;/pre&gt; &lt;p&gt;If everything goes well you&amp;#8217;ll get a successful response like this one:&lt;/p&gt; &lt;pre&gt;{"content":"Hello, World!"} &lt;/pre&gt; &lt;p&gt;Now, get your Prometheus application metrics using:&lt;/p&gt; &lt;pre&gt;$ curl ${your-openshift-application-url}/metrics &lt;/pre&gt; &lt;p&gt;You should be able to view output like what&amp;#8217;s shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_876577" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-01-1.png"&gt;&lt;img aria-describedby="caption-attachment-876577" class="wp-image-876577 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-01-1-1024x611.png" alt="Prometheus metrics for a Node.js application." width="640" height="382" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-01-1-1024x611.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-01-1-300x179.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-01-1-768x458.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-876577" class="wp-caption-text"&gt;Figure 1: Topology view from the developer perspective of the OpenShift web console.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Configuring Prometheus&lt;/h2&gt; &lt;p&gt;As of version 4.6, OpenShift comes with a built-in Prometheus instance. To use this instance, you will need to configure the monitoring stack and enable metrics for user-defined projects on your cluster, from an administrator account.&lt;/p&gt; &lt;h3&gt;Create a cluster monitoring config map&lt;/h3&gt; &lt;p&gt;To configure the core &lt;a target="_blank" rel="nofollow" href="/courses/openshift/getting-started"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; monitoring components, you must create the &lt;code&gt;cluster-monitoring-config&lt;/code&gt; &lt;code&gt;ConfigMap&lt;/code&gt; object in the &lt;code&gt;openshift-monitoring&lt;/code&gt; project. Create a YAML file called &lt;code&gt;cluster-monitoring-config.yaml&lt;/code&gt; and paste in the following:&lt;/p&gt; &lt;pre&gt; apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: | enableUserWorkload: true &lt;/pre&gt; &lt;p&gt;Then, apply the file to your OpenShift cluster:&lt;/p&gt; &lt;pre&gt;$ oc apply -f cluster-monitoring-config.yaml &lt;/pre&gt; &lt;p&gt;You also need to grant user permissions to configure monitoring for user-defined projects. Run the following command, replacing &lt;em&gt;user&lt;/em&gt; and &lt;em&gt;namespace&lt;/em&gt; with the appropriate values:&lt;/p&gt; &lt;pre&gt;$ oc policy add-role-to-user monitoring-edit &lt;em&gt;user&lt;/em&gt; -n &lt;em&gt;namespace&lt;/em&gt; &lt;/pre&gt; &lt;h3&gt;Create a service monitor&lt;/h3&gt; &lt;p&gt;The last thing to do is deploy a service monitor for your application. Deploying the service monitor allows Prometheus to scrape your application&amp;#8217;s &lt;code&gt;/metrics&lt;/code&gt; endpoint regularly to get the latest metrics. Create a file called &lt;code&gt;service-monitor.yaml&lt;/code&gt; and paste in the following:&lt;/p&gt; &lt;pre&gt; apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: k8s-app: nodeshift-monitor name: nodeshift-monitor namespace: nodeshift spec: endpoints: - interval: 30s port: http scheme: http selector: matchLabels: project: my-app &lt;/pre&gt; &lt;p&gt;Then, deploy this file to OpenShift:&lt;/p&gt; &lt;pre&gt;$ oc apply -f service-monitor.yaml &lt;/pre&gt; &lt;p&gt;The whole OpenShift monitoring stack should now be configured properly.&lt;/p&gt; &lt;h2&gt;The Prometheus dashboard&lt;/h2&gt; &lt;p&gt;With OpenShift 4.6, the Prometheus dashboard is integrated with OpenShift. To access the dashboard, go to your project and choose the &lt;b&gt;Monitoring&lt;/b&gt; item on the left, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_876537" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-02.png"&gt;&lt;img aria-describedby="caption-attachment-876537" class="wp-image-876537 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-02-1024x515.png" alt="CPU usage in the Prometheus dashboard." width="640" height="322" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-02-1024x515.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-02-300x151.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-02-768x386.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-876537" class="wp-caption-text"&gt;Figure 2: Prometheus monitoring in the OpenShift dashboard.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;To view the Prometheus metrics (using &lt;a target="_blank" rel="nofollow" href="https://prometheus.io/docs/prometheus/latest/querying/basics/"&gt;PromQL&lt;/a&gt;), go to the second tab called &lt;b&gt;Metrics&lt;/b&gt;. You can query and graph any of the metrics your application provides. For example, Figure 3 graphs the size of the heap.&lt;/p&gt; &lt;div id="attachment_876547" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-03.png"&gt;&lt;img aria-describedby="caption-attachment-876547" class="wp-image-876547 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-03-1024x515.png" alt="A heap graph in the Prometheus dashboard." width="640" height="322" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-03-1024x515.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-03-300x151.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-03-768x386.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-876547" class="wp-caption-text"&gt;Figure 3: A heap graph in Prometheus.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Testing the application&lt;/h2&gt; &lt;p&gt;Next, let&amp;#8217;s use the &lt;a target="_blank" rel="nofollow" href="https://httpd.apache.org/docs/2.4/programs/ab.html"&gt;Apache Bench&lt;/a&gt; tool to add to the load on our application. We&amp;#8217;ll hit our API endpoint 10,000 times with 100 concurrent requests at a time:&lt;/p&gt; &lt;pre&gt;$ ab -n 10000 -c 100 http://my-app-nodeshift.apps.ci-ln-5sqydqb-f76d1.origin-ci-int-gce.dev.openshift.com/api/greeting &lt;/pre&gt; &lt;p&gt;After generating this load, we can go back to the main Prometheus dashboard screen and construct a simple query to see how the service performed. We&amp;#8217;ll use our custom &lt;code&gt;http_request_duration_seconds&lt;/code&gt; metric to measure the average request duration during the last five minutes. Type this query into the textbox:&lt;/p&gt; &lt;pre&gt;rate(http_request_duration_seconds_sum[5m])/rate(http_request_duration_seconds_count[5m])&lt;/pre&gt; &lt;p&gt;Then, go to the Prometheus dashboard to see the nicely drawn graph shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_876567" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-04.png"&gt;&lt;img aria-describedby="caption-attachment-876567" class="wp-image-876567 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-04-1024x645.png" alt="Performance monitoring with Prometheus." width="640" height="403" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-04-1024x645.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-04-300x189.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-04-768x484.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-876567" class="wp-caption-text"&gt;Figure 4: Results from a custom query.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We get two lines of output because we have two types of responses: The successful one (200) and the server error (500). We can also see that as the load increases, so does the time required to complete HTTP requests.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has been a quick introduction to monitoring Node.js applications with Prometheus.  You’ll want to do much more for a production application, including setting up alerts and adding custom metrics to support &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift/nodejs-reference-architecture/blob/master/docs/operations/metrics.md#guidance"&gt;RED metrics&lt;/a&gt;. But I’ll leave those options for another article. Hopefully, this was enough to get you started and ready to learn more.&lt;/p&gt; &lt;p&gt;To learn more about what Red Hat is up to on the Node.js front, check out our new &lt;a target="_blank" rel="nofollow" href="/topics/nodejs-develop-server-side-javascript-applications"&gt;Node.js landing page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#038;title=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" data-a2a-url="https://developers.redhat.com/blog/2021/03/22/monitor-node-js-applications-on-red-hat-openshift-with-prometheus/" data-a2a-title="Monitor Node.js applications on Red Hat OpenShift with Prometheus"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/22/monitor-node-js-applications-on-red-hat-openshift-with-prometheus/"&gt;Monitor Node.js applications on Red Hat OpenShift with Prometheus&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Lh6Nke4fASw" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;A great thing about Node.js is how well it performs inside a container. With the shift to containerized deployments and environments comes extra complexity. One such complexity is observing what’s going on within your application and its resources, and when resource use is outside of the expected norms. Prometheus is a tool that developers can [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/22/monitor-node-js-applications-on-red-hat-openshift-with-prometheus/"&gt;Monitor Node.js applications on Red Hat OpenShift with Prometheus&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/22/monitor-node-js-applications-on-red-hat-openshift-with-prometheus/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">876397</post-id><dc:creator>Alexandros Alykiotis</dc:creator><dc:date>2021-03-22T07:00:23Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/22/monitor-node-js-applications-on-red-hat-openshift-with-prometheus/</feedburner:origLink></entry></feed>
